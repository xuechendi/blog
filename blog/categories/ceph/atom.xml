<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Ceph | XueChendi]]></title>
  <link href="http://xuechendi.github.io/blog/categories/ceph/atom.xml" rel="self"/>
  <link href="http://xuechendi.github.io/"/>
  <updated>2013-12-04T21:02:00+08:00</updated>
  <id>http://xuechendi.github.io/</id>
  <author>
    <name><![CDATA[Chendi.Xue]]></name>
    <email><![CDATA[xuechendi@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[AIO-STRESS vs. FIO]]></title>
    <link href="http://xuechendi.github.io/blog/2013/11/15/aio-stress-vs-fio/"/>
    <updated>2013-11-15T01:05:00+08:00</updated>
    <id>http://xuechendi.github.io/blog/2013/11/15/aio-stress-vs-fio</id>
    <content type="html"><![CDATA[<p>There is no big gap to me in producing workload by AIO-STREE and FIO.</p>

<p>Just because I haven&rsquo;t using much more advanced feature in FIO like replay trace, and I just using libaio as the driver.</p>

<p>But here, I just wanna show some interesting test to see some behavior by different setting of io_depth and io_batch. And <code>sadly</code> that is just some wrong setting I did before and produced some strange data then I just realized how these settings mean. T_T</p>

<h2>Just follow my lead to see how I found this wrong setting. ^_^</h2>

<p><img src="/images/AIO_vs_FIO/1.png" alt="findings" /></p>

<p>After a loadline test of io_depth setting, I just found the figures followed some rules, so using qemu logs, I draw above graph. This is a graph recording the io request intervals sending from qemu to librbd, and catched in librbd logs.</p>

<p>As we can see, when we set the queue_depth as 8, the first io request has quite small interval(0.075ms), but the interval between the 8th and 9th io request is pretty long(1.6ms). And if we set the queue_depth as 16, or 32. The burst interval happened just between the 16th to 17th, and 32th to 33th respectively.</p>

<p>So there comes the deduction:
Queue will send out all requests, then wait the last request completes . After that it starts to send out new queue.</p>

<h2>How to prove it?</h2>

<h3>Methodology:</h3>

<ol>
<li>. Set queue depth = 64</li>
<li>. make the 42th request sleep 3s in completion thread</li>
</ol>


<h3>&mdash;> If RBD waits 3s then receives the 65th requests, we can confirm the deduction.</h3>

<h3>How to do this?</h3>

<p>There is a function in librbd named C_AioRead::finish in AioCompletion.cc, so what we need to do is just randomly block this function for 3s, and see what happens.</p>

<p><img src="/images/AIO_vs_FIO/2.png" alt="proving" /></p>

<p>Aha! The results just show as what we expect.</p>

<h3>But there are still 3 possibilities can produce result like this.</h3>

<p>1.Wrong workload setting?
2.Ceph RBD design ?  if there is some lock in finish thread that blocks the submit thread
3.Virtio ?</p>

<p>The situation produced by FIO</p>

<p><img src="/images/AIO_vs_FIO/3.png" alt="proving" /></p>

<p>From this graph, we can see the result is really different by using aio-stress, which can help us to exclude last two possibilities. That means we must do some wrong setting in aio-stress or aio-stress just have some wierd strategy design.</p>

<p>Also we can see the difference in logs.</p>

<p><img src="/images/AIO_vs_FIO/4.png" alt="proving" /></p>

<p><code>Here I will add some aio-stress codes digging into soon, just not now~ Sorry</code></p>

<hr>


<p>The right setting for random io in aio-stress to act more meet our demand (Here I said random io is because this setting may result in no io merge in sequential io)</p>

<p><code>console
./aio-stress -O -o 3 -i 1 -d 64 -r4k -s 4m /dev/vdb
</code></p>

<p><img src="/images/AIO_vs_FIO/5.png" alt="proving" /></p>

<p><img src="/images/AIO_vs_FIO/6.png" alt="proving" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to break into Ceph Codes]]></title>
    <link href="http://xuechendi.github.io/blog/2013/11/15/how-to-break-into-ceph-codes/"/>
    <updated>2013-11-15T01:00:00+08:00</updated>
    <id>http://xuechendi.github.io/blog/2013/11/15/how-to-break-into-ceph-codes</id>
    <content type="html"><![CDATA[<p>Working in Progress, will be finished by this weekend</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ObjectCacher: RBD Cache Codes]]></title>
    <link href="http://xuechendi.github.io/blog/2013/11/15/objectcacher-rbd-cache-codes/"/>
    <updated>2013-11-15T00:58:00+08:00</updated>
    <id>http://xuechendi.github.io/blog/2013/11/15/objectcacher-rbd-cache-codes</id>
    <content type="html"><![CDATA[<p>There is some cache mechanism in Ceph RBD.
And it is now in memory cache which supports write through and write back.</p>

<p>In this post, I would post what I have learnt from ObjectCacher codes.</p>

<h2>How to deploy RBD Cache</h2>

<p>there is a really clear instruction in ceph.com, just linked here <code><a href="http://ceph.com/docs/master/rbd/rbd-config-ref/?highlight=rbd%20cache">Ceph RBD Cache setting</a></code></p>

<h2>How to record log from RBD</h2>

<p>There is a little tricky to record log in RBD sides, because if you simply add the log settings in ceph.conf, you can only get logs when you do something using &ldquo;rbd&rdquo; command, like &ldquo;rbd lspools&rdquo;, etc. So simply adding log settings in ceph.conf(client side) can not help you to get logs from QEMU to librbd, the reason is unknown to me, but I just find a way to walk around this.</p>

<p>All you need to do is to add log setting in you instance.xml, then using libvirt to boot this instance or also you can just attach a new disk by using xml like below.</p>

<p>``` xml disk.xml</p>

<pre><code>&lt;disk type='network' device='disk'&gt;
  &lt;driver name='qemu' type='raw' cache='none'/&gt;
  &lt;source protocol='rbd' name='xcd_8osd/volume-3:debug_rbd=20:debug_objectcacher=20:log_file=/tmp/qemu-rbd.log'/&gt;
  &lt;target dev='vdb' bus='virtio'/&gt;
  &lt;serial&gt;12f70341-d199-4fca-9270-56e5d6b80061&lt;/serial&gt;
  &lt;alias name='virtio-disk1'/&gt;
  &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/&gt;
&lt;/disk&gt;
</code></pre>

<p>```</p>

<p>After attach this disk, you can find logs in /tmp/qemu-rbd.log or you can just write to qemu log.</p>

<p>``` xml write log to /var/libvirt/qemu/xxxxx(instance_name).log</p>

<pre><code>  &lt;source protocol='rbd' name='xcd_8osd/volume-3:debug_rbd=20:debug_objectcacher=20:log_to_stderr=true'/&gt;
</code></pre>

<p>```
There is a really clear codes path we can find from logs, so just using log to debug or hack into the rbd codes.</p>

<h2>Dig into the codes</h2>

<p>If we enable rbd cache in ceph.conf(client side), the ObjectCacher object is created when this rbd image attach to the vm.
Here is the overview.</p>

<p><img src="/images/RBD_Cache/overview.png" width=60%></p>

<p>From above we can see each RBD has a object to store all information name ImageCtx(image context), so there is only one ObjectCacher in one RBD cache and also different RBD images can not share their cache till now.</p>

<p>ObjectCacher uses poolid, oid(objectid), then offset and length to index cache in memory.</p>

<p>Then here is a graph to show how ceph using ObjectCacher.</p>

<p><img src="/images/RBD_Cache/workflow_send_req.png" width=100%></p>

<p><img src="/images/RBD_Cache/workflow_recv_req.png" width=50%></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Study of Data distribution methods--Crush]]></title>
    <link href="http://xuechendi.github.io/blog/2013/11/15/crush-ceph-object-distribution-method/"/>
    <updated>2013-11-15T00:51:00+08:00</updated>
    <id>http://xuechendi.github.io/blog/2013/11/15/crush-ceph-object-distribution-method</id>
    <content type="html"><![CDATA[<p>wanna learn, mark here</p>
]]></content>
  </entry>
  
</feed>
