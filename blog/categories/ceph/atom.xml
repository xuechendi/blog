<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Ceph | XueChendi]]></title>
  <link href="http://xuechendi.github.io/blog/categories/ceph/atom.xml" rel="self"/>
  <link href="http://xuechendi.github.io/"/>
  <updated>2014-08-22T04:56:50+08:00</updated>
  <id>http://xuechendi.github.io/</id>
  <author>
    <name><![CDATA[Chendi.Xue]]></name>
    <email><![CDATA[xuechendi@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Dive into Ceph Write IO Path]]></title>
    <link href="http://xuechendi.github.io/blog/2013/12/18/dive-into-ceph-write-io-path/"/>
    <updated>2013-12-18T14:41:00+08:00</updated>
    <id>http://xuechendi.github.io/blog/2013/12/18/dive-into-ceph-write-io-path</id>
    <content type="html"><![CDATA[<p><em>After talking with a friend of mine who is really intelligent and awesome and intelligent, just thinking about to post this article to get through the Ceph OSD Write IO path. There I am sorry to say must exists some misunderstanding in this article, but I hope it will still means something for people who read this article&hellip;</em></p>

<p><em>Ok, start here.</em></p>

<h6>tip: I am not planning finish this article in a fast way, for I am trying to make it more meaninhful.</h6>

<p>======</p>

<h2>Content</h2>

<ul>
<li>Ceph Write IO Path overview</li>
<li>3 phase codes deep dive</li>
<li>Throttle and lock</li>
<li>Latency breakdown</li>
</ul>


<h2>Referrence:</h2>

<ul>
<li><a href="https://ceph.com/docs/master/dev/object-store/">OBJECT STORE ARCHITECTURE OVERVIEW</a></li>
<li><a href="https://ceph.com/docs/master/dev/osd_internals/map_message_handling/">MAP AND PG MESSAGE HANDLING</a></li>
<li><a href="http://ceph.com/docs/master/dev/osd_internals/osd_throttles">OSD Internals</a></li>
<li><a href="https://ceph.com/docs/master/dev/osd_internals/osd_overview/">OSD</a></li>
</ul>


<p>There is a IO path graph on ceph.com, pls click <code><a href="http://ceph.com/docs/master/dev/osd_internals/osd_throttles/?highlight=wbthrottle">OSD Internals</a></code>, I find the graph is a bit difficult to see, so redraw it in a vertical way</p>

<div style="float:left;width:100%">
<div style="float:left; width:58%;">
    <div style="border-left:red solid 5px; padding-left:5%; ">
    <h3>Queue and thread overview</h3>
        <h5>Will finish today</h5>
    </div>
</div>

<img src="/images/ceph_write_path/io_path_1.png" style="margin-left:5%; width:35%; border:5px solid #222; float:left;">
</div>


<h3>Below I will describe the write io path by dividing into 3 parts, Dispatch write io msg, write to journal, write to filestore :</h3>

<h3>DispatchThread</h3>

<div style="font-size:12px;  border: solid #999 1px; background-color: #eee; width:100%; overflow: scroll; padding:5px; white-space:nowrap;text-overflow:scroll;">
1:2013-12-25 15:20:27.824912 7f8fda903700 20 <red>osd.12</red> 344 </red>_dispatch</red> 0x55803b40 <red>osd_op</red>(client.6219.0:8 rbd_data.10126b8b4567.0000000000000000 [write 0~512] 3.ff00f0a2 e344) v4<br>
2:2013-12-25 15:20:27.824935 7f8fda903700 15 <red>osd.12</red> 344 <red>enqueue_op</red> 0x54e7cb60 prio 63 cost 512 latency 0.000163 <red>osd_op</red>(client.6219.0:8 rbd_data.10126b8b4567.0000000000000000 [write 0~512] 3.ff00f0a2 e344) v4<br>
</div>


<p><code>c DispatchThread
qitem = mqueue.dequeue() //mqueue is enqueued by Pipe::Reader, it stores every msg received by this osd.
m = qitem.get_message()
msgr→ms_deliver_dispatch(m)
p = dispatchers.begin()
p→ms_dispatch(m)  =&gt; OSD::ms_dispatch(m) // find a dispatcher can handle this msg
_dispatch(m) // OSD.cc, log above is printed in this function
dispatch_op(op) // switch the MSG type in this function, and in this case, msg-&gt;type would be "CEPH_MSG_OSD_OP"
handle_op(op) //do some preparation including refresh the osd map
enqueue_op(pg, op)
op_wq.queue(pg, op) //When OSD daemon starts, an OSD instance will be created, here calls the osd-&gt;op_wq-&gt;_enqueue(pair&lt;...&gt;) to put the op into osd-&gt;op_wq-&gt;pqueue
</code></p>

<h3>OSD:OpWQ</h3>

<div style="font-size:12px;  border: solid #999 1px; background-color: #eee; width:100%; overflow: scroll; padding:5px; white-space:nowrap;text-overflow:scroll;">
3:2013-12-25 15:20:27.824972 7f8fc70dc700 12 OSD::op_tp worker wq <red>OSD::OpWQ</red> <red>start processing</red> 0x1 (1 active)<br>
4:2013-12-25 15:20:27.824989 7f8fc70dc700 10 <red>osd.12</red> 344 <red>dequeue_op</red> 0x54e7cb60 prio 63 cost 512 latency 0.000217 osd_op(client.6219.0:8 rbd_data.10126b8b4567.0000000000000000 <red>[write 0~512]</red> 3.ff00f0a2 e344) v4 pg pg[3.a2( v 344'3679 (18'678,344'3679] local-les=338 n=3 ec=15 les/c 338/342 337/337/334) [12,13] r=0 lpr=337 mlcod 344'3678 active+clean]<br>
5:2013-12-25 15:20:27.825069 7f8fc70dc700 10 <red>osd.12</red> pg_epoch: 344 pg[3.a2( v 344'3679 (18'678,344'3679] local-les=338 n=3 ec=15 les/c 338/342 337/337/334) [12,13] r=0 lpr=337 mlcod 344'3678 active+clean] <red>do_op</red> osd_op(client.6219.0:8 rbd_data.10126b8b4567.0000000000000000 <red>[write 0~512]</red> 3.ff00f0a2 e344) v4 <red>may_write -> write-ordered</red><br>
7:2013-12-25 15:20:27.825284 7f8fc70dc700 10 <red>osd.12</red> pg_epoch: 344 pg[3.a2( v 344'3679 (18'678,344'3679] local-les=338 n=3 ec=15 les/c 338/342 337/337/334) [12,13] r=0 lpr=337 mlcod 344'3678 active+clean] <red>execute_ctx</red> 0x5548d800<br>
8:2013-12-25 15:20:27.825302 7f8fc70dc700 10 <red>osd.12</red> pg_epoch: 344 pg[3.a2( v 344'3679 (18'678,344'3679] local-les=338 n=3 ec=15 les/c 338/342 337/337/334) [12,13] r=0 lpr=337 mlcod 344'3678 active+clean] <red>do_op</red> ff00f0a2/rbd_data.10126b8b4567.0000000000000000/head//3 [write 0~512] ov 344'3679 av 344'3680 snapc 0=[] snapset 0=[]:[]+head<br>
9:2013-12-25 15:20:27.825322 7f8fc70dc700 10 <red>osd.12</red> pg_epoch: 344 pg[3.a2( v 344'3679 (18'678,344'3679] local-les=338 n=3 ec=15 les/c 338/342 337/337/334) [12,13] r=0 lpr=337 mlcod 344'3678 active+clean] <red>do_osd_op</red> ff00f0a2/rbd_data.10126b8b4567.0000000000000000/head//3 [write 0~512]<br>
10:2013-12-25 15:20:27.825338 7f8fc70dc700 10 <red>osd.12</red> pg_epoch: 344 pg[3.a2( v 344'3679 (18'678,344'3679] local-les=338 n=3 ec=15 les/c 338/342 337/337/334) [12,13] r=0 lpr=337 mlcod 344'3678 active+clean] <red>do_osd_op</red>  write 0~512<br>
11:2013-12-25 15:20:27.825361 7f8fc70dc700 15 <red>osd.12</red> pg_epoch: 344 pg[3.a2( v 344'3679 (18'678,344'3679] local-les=338 n=3 ec=15 les/c 338/342 337/337/334) [12,13] r=0 lpr=337 mlcod 344'3678 active+clean] <red>do_osd_op_effects</red> on session 0x54fb7d20<br>
12:2013-12-25 15:20:27.825653 7f8fc70dc700 10 <red>osd.12</red> pg_epoch: 344 pg[3.a2( v 344'3680 (18'679,344'3680] local-les=338 n=3 ec=15 les/c 338/342 337/337/334) [12,13] r=0 lpr=337 luod=344'3679 lcod 344'3679 mlcod 344'3678 active+clean] <red>new_repop</red> rep_tid 24 on osd_op(client.6219.0:8 rbd_data.10126b8b4567.0000000000000000 [write 0~512] 3.ff00f0a2 e344) v4<br>
13:2013-12-25 15:20:27.825678 7f8fc70dc700  7 <red>osd.12</red> pg_epoch: 344 pg[3.a2( v 344'3680 (18'679,344'3680] local-les=338 n=3 ec=15 les/c 338/342 337/337/334) [12,13] r=0 lpr=337 luod=344'3679 lcod 344'3679 mlcod 344'3678 active+clean] <red>issue_repop</red> rep_tid 24 o ff00f0a2/rbd_data.10126b8b4567.0000000000000000/head//3<br>
14:2013-12-25 15:20:27.825724 7f8fc70dc700 10 <red>osd.12</red> pg_epoch: 344 pg[3.a2( v 344'3680 (18'679,344'3680] local-les=338 n=3 ec=15 les/c 338/342 337/337/334) [12,13] r=0 lpr=337 luod=344'3679 lcod 344'3679 mlcod 344'3678 active+clean] <red>eval_repop</red> repgather(0x55842780 344'3680 rep_tid=24 wfack=12,13 wfdisk=12,13 op=osd_op(client.6219.0:8 rbd_data.10126b8b4567.0000000000000000 [write 0~512] 3.ff00f0a2 e344) v4) wants=ad<br>
15:2013-12-25 15:20:27.825759 7f8fc70dc700 10 <red>osd.12</red> pg_epoch: 344 pg[3.a2( v 344'3680 (18'679,344'3680] local-les=338 n=3 ec=15 les/c 338/342 337/337/334) [12,13] r=0 lpr=337 luod=344'3679 lcod 344'3679 mlcod 344'3678 active+clean] <red>apply_repop</red>  applying update on repgather(0x55842780 344'3680 rep_tid=24 wfack=12,13 wfdisk=12,13 op=osd_op(client.6219.0:8 rbd_data.10126b8b4567.0000000000000000 [write 0~512] 3.ff00f0a2 e344) v4)<br>
16:2013-12-25 15:20:27.825784 7f8fc70dc700  5 filestore(/data/osd.12) <red>queue_transactions</red> existing osr(3.a2 0x18b7840)/0x18b7840<br>
17:2013-12-25 15:20:27.825802 7f8fc70dc700  5 filestore(/data/osd.12) <red>queue_transactions</red> (writeahead) 3457004 0x5548dbf0,0x5548db38<br>
18:2013-12-25 15:20:27.825807 7f8fc70dc700 10 journal <red>op_journal_transactions</red> 3457004 0x5548dbf0,0x5548db38<br>
19:2013-12-25 15:20:27.825840 7f8fc70dc700 10 <red>osd.12</red> 344 <red>dequeue_op</red> 0x54e7cb60 finish<br>
20:2013-12-25 15:20:27.825843 7f8fe003b700 20 journal <red>write_thread_entry</red> <red>woke up</red><br>
21:2013-12-25 15:20:27.825846 7f8fc70dc700 15 OSD::op_tp worker wq <red>OSD::OpWQ</red> <red>done processing</red> 0x1 (0 active)<br>
</div>


<p>```c OSD:OpWQ
/<em>ThreadPool::worker is a thread looping all the time to dequeue and process items of workerqueue in his threadpool, codes in WorkQueue.cc</em>/
wq = work_queues[last_work_queue]; //wq is osd->op_wq here
void <em>item = wq-><em>void_dequeue(); //osd->op_wq-></em>dequeue() dequeue one item from osd->op_wq->pqueue, and push into pg_for_processing[&amp;</em>pg]
wq-><em>void_process(item, tp_handle); //osd->op_wq-></em>process(pg) pop item from pg_for_processing[&amp;*pg]
osd→dequeue_op(pg, op); //log above taged by &ldquo;dequeue_op&rdquo; is printed here. it calls do_request function, then print another &ldquo;dequeue_op&rdquo; msg.
pg→do_request(op); //implemented in ReplicatedPG.cc, switch the op type and calls do_op;
do_op(op); // Initiate the OpContext ctx, then calls execute_ctx(ctx);</p>

<p>execute_ctx(ctx);</p>

<pre><code>prepare_transaction(ctx);
    do_osd_ops(ctx, ctx-&gt;ops); //switch (op.op), if read, call osd-&gt;store-&gt;read; if write, call ObjectStore::Transaction::write() prepare the transaction.
ctx-&gt;reply = new MOSDOpReply(m, 0, get_osdmap()-&gt;get_epoch(), 0); //Write operations aren't allowed to return a data payload
RepGather *repop = new_repop(ctx, obc, rep_tid); //replica operation
issue_repop(repop, now); //add itself and all the replica peer into repop-&gt;wairfor_disk and repop-&gt;waitfor_ack; 
    for peer in itsself and its peers:
        repop-&gt;waitfor_ack.insert(peer);
        repop-&gt;waitfor_disk.insert(peer);
    MOSDSubOp *wr = new MOSDSubOp(...)
    osd-&gt;send_message_osd_cluster(peer, wr, get_osdmap()-&gt;get_epoch()); //send supop to its peers
eval_repop(repop);
    apply_repop(repop); //if the writing to itself hasn't been applied, this function will put the transactions into osd-&gt;store-&gt;journal-&gt;writeq(talks about this later)
    if(repop-&gt;waitfor_disk.empty()) send commit msg
    if(repop-&gt;waitfor_ack.empty()) send ack msg
    if(repop-&gt;waitfor_ack.empty() &amp;&amp; repop-&gt;waitfor_disk.empty() &amp;&amp; repop-&gt;applied) remove(repop) 
</code></pre>

<p>//then back to discuss the apply_repop function.
apply_repop(repop)</p>

<pre><code>//Set the completion action will be taken when this operation finishes here.
Context *oncommit = new C_OSD_OpCommit(this, repop);
Context *onapplied = new C_OSD_OpApplied(this, repop);
Context *onapplied_sync = new C_OSD_OndiskWriteUnlock(...);
osd→store→queue_transactions(…) //apply_repop calls queue_transactions function, and because when osd instance first initiated, store is created as a FileStore instance in funtion OSD::pre_init(), so here, we actually calls the FileStore::queue_transactions();
</code></pre>

<p>FileStore::queue_transactions()</p>

<pre><code>osr-&gt;queue_journal(o-&gt;op);
_op_journal_transactions(o-&gt;tls, o-&gt;op,new C_JournaledAhead(this, osr, o, ondisk),osd_op); //C_JournaledAhead::finish() will be called when this transaction finishes.
</code></pre>

<p>_op_journal_transactions(&hellip;)
journal->submit_entry(op, tbl, data_align, onjournal, osd_op); //journal is osd->store->journal here</p>

<pre><code>completions.push_back(completion_item(...)); //a completions queue, store the completion instance (which tells what action should be taken when this writing finished).
writeq.push_back(write_item(seq, e, alignment, osd_op)); //put the osd_op into osd-&gt;store-&gt;journal-&gt;writeq
writeq_cond.Signal(); //wake up here writeq_cond.Wait() is =&gt; FileJournal::write_thread_entry()
</code></pre>

<p>osd-><em>process_finish() //ThreadPool::worker </em>void_process_finish , this function mainly erase the in_use info of pgs.</p>

<p>//Ok, the worker_queue(osd->op_wq) finishes a whole loop till now, and back to get a new request.<br/>
```</p>

<h3>FileJournal</h3>

<div style="font-size:12px;  border: solid #999 1px; background-color: #eee; width:100%; overflow: scroll; padding:5px; white-space:nowrap;text-overflow:scroll;">
20:2013-12-25 15:20:27.825843 7f8fe003b700 20 journal <red>write_thread_entry</red> woke up<br>
22:2013-12-25 15:20:27.825856 7f8fe003b700 15 journal <red>prepare_single_write</red> 1 will write 10288709632 : seq 3457004 len 2577 -> 4096 (head 40 pre_pad 0 ebl 2577 post_pad 1439 tail 40) (ebl alignment -1)<br>
23:2013-12-25 15:20:27.825882 7f8fe003b700 15 journal <red>do_aio_write</red> writing 10288709632~4096 + header<br>
24:2013-12-25 15:20:27.825884 7f8fe003b700 20 journal <red>write_aio_bl</red> 0~4096 seq 0<br>
25:2013-12-25 15:20:27.825887 7f8fe003b700 20 journal <red>write_aio_bl</red> .. 0~4096 in 1<br>
26:2013-12-25 15:20:27.825940 7f8fe003b700 20 journal <red>write_aio_bl</red> 10288709632~4096 seq 3457004<br>
27:2013-12-25 15:20:27.825944 7f8fe003b700 20 journal <red>write_aio_bl</red> .. 10288709632~4096 in 1<br>
28:2013-12-25 15:20:27.826359 7f8fe003b700 20 journal <red>write_thread_entry</red> going to sleep<br>
29:2013-12-25 15:20:27.826340 7f8fdf83a700 20 journal <red>write_finish_thread_entry</red> waiting for aio(s)<br>
30:2013-12-25 15:20:27.826365 7f8fdf83a700 10 journal <red>write_finish_thread_entry</red> aio 0~4096 done<br>
31:2013-12-25 15:20:27.826369 7f8fdf83a700 20 journal <red>check_aio_completion</red><br>
32:2013-12-25 15:20:27.826371 7f8fdf83a700 20 journal <red>check_aio_completion</red> completed seq 0 0~4096<br>
33:2013-12-25 15:20:27.826376 7f8fdf83a700 20 journal <red>write_finish_thread_entry</red> waiting for aio(s)<br>
34:2013-12-25 15:20:27.826800 7f8fdf83a700 10 journal <red>write_finish_thread_entry</red> aio 10288709632~4096 done<br>
35:2013-12-25 15:20:27.826806 7f8fdf83a700 20 journal <red>check_aio_completion</red><br>
36:2013-12-25 15:20:27.826807 7f8fdf83a700 20 journal <red>check_aio_completion</red> completed seq 3457004 10288709632~4096<br>
37:2013-12-25 15:20:27.826811 7f8fdf83a700 20 journal <red>check_aio_completion</red> queueing finishers through seq 3457004<br>
38:2013-12-25 15:20:27.826813 7f8fdf83a700 10 journal <red>queue_completions_thru</red> seq 3457004 queueing seq 3457004 0x55df22a0 lat 0.000987<br>
39:2013-12-25 15:20:27.826832 7f8fdf83a700 20 journal <red>write_finish_thread_entry</red> sleeping<br>
40:2013-12-25 15:20:27.826841 7f8fdf039700 10 finisher(0x18e7018) <red>finisher_thread</red> doing [0x55df22a0]<br>
41:2013-12-25 15:20:27.826849 7f8fdf039700  5 filestore(/data/osd.12) <red>_journaled_ahead</red> 0x54ff4370 seq 3457004 osr(3.a2 0x18b7840) 0x5548dbf0,0x5548db38<br>
42:2013-12-25 15:20:27.826856 7f8fdf039700  5 filestore(/data/osd.12) <red>queue_op</red> 0x54ff4370 seq 3457004 osr(3.a2 0x18b7840) 2565 bytes   (queue has 1 ops and 2565 bytes)<br>
43:2013-12-25 15:20:27.826869 7f8fdf039700 10 finisher(0x18e7018) <red>finisher_thread</red> done with [0x55df22a0]<br>
44:2013-12-25 15:20:27.826873 7f8fdf039700 10 finisher(0x18e7018) <red>finisher_thread</red> empty<br>
45:2013-12-25 15:20:27.826875 7f8fdf039700 10 finisher(0x18e7018) <red>finisher_thread</red> sleeping<br>
</div>


<p>```c FileJournal::Writer
FileJournal::write_thread_entry(); //with/without aio will be different here.</p>

<pre><code>r = prepare_multi_write(bl, orig_ops, orig_bytes); //r = prepare_single_write(bl, queue_pos, orig_ops, orig_bytes); pop item from writeq and push to journalq, journalq is to be used when need to sync.
if (aio) write_aio_bl(pos, second, writing_seq);
//if using aio, then will be like below
    io_prep_pwritev(&amp;aio.iocb, fd, aio.iov, n, pos);
    io_submit(aio_ctx, 1, &amp;piocb);
    write_finish_cond.Signal(); //write_finish_thread will wake up here
    /*==============================*/
    write_finish_thread_entry();
        check_aio_completion(); //check each aio_info in the aio_queue(we push each write io into aio_queue in write_aio_bl function)
        queue_completions_thru(journaled_seq); 
    /*==============================*/
    else do_write(bl);
//if not using aio, will be like below
    writev(fd, start, num);
    fdatasync(fd);
    queue_completions_thru(journaled_seq);
</code></pre>

<p>queue_completions_thru(journaled_seq);</p>

<pre><code>completion_item next = completion_peek_front(); //get item from completions
finisher-&gt;queue(next.finish); put the completion.finish =&gt; C_JournaledAhead::finish() in finisher thread
finisher_cond.Signal(); //wake up osd-&gt;store-&gt;journal-&gt;finisher thread, then calls fs-&gt;_journaled_ahead(osr, o, ondisk);
</code></pre>

<p>FileStore::_journaled_ahead(OpSequencer <em>osr, Op </em>o, Context <em>ondisk)
FileStore::queue_op(OpSequencer </em>osr, Op *o)
osd->store->op_wq.queue(osr); //Notice here, it calls the osd->store->op_wq.queue not osd->op_wq, and actually the io unit enqueue the osd->store->op_queue
ondisk_finisher.queue(ondisk); //wake up the Finisher::finisher_thread_entry() which calls C_OSD_OpCommit.finish() => pg->op_commit(repop);</p>

<pre><code>repop-&gt;waitfor_disk.erase(whoami);
repop-&gt;waitfor_ack.erase(whoami);
eval_repop(repop); //check if this rep_op can send its commit and ack
</code></pre>

<p>//Ok, Till now, Journal writing process is finished, the filestore write just begins.
//Another thing is that, if we use btrfs, then in function queue_transactions, the write to journal and filestore will be parallel.
```</p>

<p>FileStore::OpWQ</p>

<div style="font-size:12px;  border: solid #999 1px; background-color: #eee; width:100%; overflow: scroll; padding:5px; white-space:nowrap;text-overflow:scroll;">

</div>


<p>```c FileStore::OpWQ
store-><em>do_op(osr, handle); //ThreadPool::worker wq-></em>void_process(item, tp_handle);
<em>do_transactions(o->tls, o->op, &amp;handle);
</em>do_transaction(**(o->tls->begin), op_seq, trans_num, handle);</p>

<pre><code>switch(op) case Transaction::OP_WRITE: 
    decode cid, oid, off, len, bl from osr-&gt;q-&gt;tls
    r = _write(cid, oid, off, len, bl, replica);
        FileStore::_write(...) =&gt; writev
</code></pre>

<p>store-><em>do_op(osr, handle); //ThreadPool::worker wq-></em>void_process_finish(item);
store->_finish_op(osr)</p>

<pre><code>if (o-&gt;onreadable_sync) o-&gt;onreadable_sync-&gt;complete(0); //C_OSD_OndiskWriteUnlock.finish()
op_finisher.queue(o-&gt;onreadable); // wake up the Finisher::finisher_thread_entry() and C_OSD_OpApplied.finish() =&gt; pg-&gt;op_applied(repop);
</code></pre>

<p>pg->op_applied(repop)</p>

<pre><code>repop-&gt;applying = false;
repop-&gt;applied = true;
repop-&gt;waitfor_ack.erase(whoami);
eval_repop(repop); //check if this rep_op can send its commit and ack
</code></pre>

<p>```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ceph Best known Performance Collection]]></title>
    <link href="http://xuechendi.github.io/blog/2013/12/13/ceph-best-known-performance-collection/"/>
    <updated>2013-12-13T15:35:00+08:00</updated>
    <id>http://xuechendi.github.io/blog/2013/12/13/ceph-best-known-performance-collection</id>
    <content type="html"><![CDATA[<p>In this posting, I hope to collect  <em>TILL NOW BEST KNOWN</em>  Ceph Performance and also their deployment/setup method.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Building Ceph]]></title>
    <link href="http://xuechendi.github.io/blog/2013/12/12/building-ceph/"/>
    <updated>2013-12-12T11:17:00+08:00</updated>
    <id>http://xuechendi.github.io/blog/2013/12/12/building-ceph</id>
    <content type="html"><![CDATA[<p>Actually, I had bulid ceph from codes for many times, just each time, I have to struggle a little bit, for I always forget install some libraries.</p>

<p>So, record things here</p>

<p>I found ceph.com have also detailed its &ldquo;Building from Codes&rdquo; doc, indeed, much detailed than previous version</p>

<p>But seems I still missing some packages when doing &ldquo;make&rdquo;</p>

<p><code><a href="http://ceph.com/docs/master/install/build-ceph/">http://ceph.com/docs/master/install/build-ceph/</a></code></p>

<p>```console pre-make-ceph-packages
sudo apt-get install pkg-config autotools-dev autoconf automake cdbs gcc g++ git libboost-dev libedit-dev libssl-dev libtool libfcgi libfcgi-dev libfuse-dev linux-kernel-headers libcrypto++-dev libcrypto++ libexpat1-dev libboost-thread-dev uuid-dev libkeyutils-dev libgoogle-perftools-dev libatomic-ops-dev libaio-dev libgdata-common libgdata13 libsnappy-dev libleveldb-dev libboost-program-options-dev
cd ceph
./autogen.sh
./configure
make
make install</p>

<p>```</p>

<p><strong><strong><strong><strong><strong><strong><strong><strong><em>Extra parts</em></strong></strong></strong></strong></strong></strong></strong></strong>****
I have ran into the libboost-thread and libboost-system problem for really a lot of times, so the best way to do here is that:</p>

<h2>Keep same and the only version of libboost dpkg installed in your system ( verified by dpkg -l | grep boost )</h2>

<h2>Any time after your purge/remove and install pkg, don&rsquo;t just try make, you need to do ./configure first</h2>

<h2>Oh, maybe problem also can be caused by you have multi gcc/g++ version</h2>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to do Ceph Latency Breakdown]]></title>
    <link href="http://xuechendi.github.io/blog/2013/12/05/how-to-do-ceph-latency-breakdown/"/>
    <updated>2013-12-05T10:38:00+08:00</updated>
    <id>http://xuechendi.github.io/blog/2013/12/05/how-to-do-ceph-latency-breakdown</id>
    <content type="html"><![CDATA[<h2>The Ceph Latency Breakdown process includes 3 steps:</h2>

<h3>1. Enable Log Configuration or Other performance count mechanism to record the runtime infomation.</h3>

<h3>2. Produce the workload and meanwhile dump the perfcounter</h3>

<h3>3. Parse the perfcounter dump data</h3>

<h2>Details:</h2>

<p>In ceph, a perfcounter mechanism has been adopted to do the latency breakdown, and the runtime info can be dumped by the admin_socket is an internal implementation in Ceph.</p>

<p>example:</p>

<p><1> codes
``` c git diff
diff &mdash;git a/src/osd/OSD.cc b/src/osd/OSD.cc
index d34bb9c..6c308d3 100644
&mdash;&ndash; a/src/osd/OSD.cc
+++ b/src/osd/OSD.cc
@@ -7055,10 +7055,10 @@ PGRef OSD::OpWQ::_dequeue()</p>

<p> void OSD::OpWQ::_process(PGRef pg, ThreadPool::TPHandle &amp;handle)
 {
&ndash;  utime_t pg_lock_lat = ceph_clock_now(g_ceph_context);
+  utime_t start = ceph_clock_now(g_ceph_context);
   pg->lock_suspend_timeout(handle);
&ndash;  pg_lock_lat = ceph_clock_now(g_ceph_context) &ndash; pg_lock_lat;
&ndash;  osd->logger->tinc(l_osd_pg_lock_lat, pg_lock_lat);
+  utime_t end = ceph_clock_now(g_ceph_context);
+  osd->logger->tinc(l_osd_pg_lock_lat, end &ndash; start);</p>

<p>   OpRequestRef op;
   {
```</p>

<p>Current Ceph Version only enabled admin_socket in Ceph Cluster Side, so we need to enable it in RBD side.</p>

<p><2> dump the data</p>

<p>``` bash ceph.conf</p>

<h1>Enable the admin_socket in QEMU RBD</h1>

<p>check ceph.conf
perf = false
&ndash;></p>

<h1>perf = false</h1>

<p>```</p>

<p>```xml QEMU disk.xml</p>

<pre><code>&lt;disk type='network' device='disk'&gt;
  &lt;driver name='qemu' type='raw' cache='none'/&gt;
  &lt;source protocol='rbd' name='xcd_8osd/volume-3:admin_socket=/var/run/ceph/node5_1.asok'/&gt;
  &lt;target dev='vdb' bus='virtio'/&gt;
  &lt;serial&gt;12f70341-d199-4fca-9270-56e5d6b80061&lt;/serial&gt;
  &lt;alias name='virtio-disk1'/&gt;
  &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/&gt;
&lt;/disk&gt;
</code></pre>

<p><code>
</code> console
virsh attach instance1 disk.xml</p>

<p>ceph &mdash;admin-daemon /var/run/ceph/${osd} perf dump >> ${osd}.txt
```</p>

<hr>


<h2>Below is the summary of all scripts:</h2>

<p><a href ="/downloads/Ceph_Latency_Breakdown_Script.zip"><code style="font-family: 'Fjalla One','Georgia','Helvetica Neue',Arial,sans-serif;font-weight:900;font-size:12px;">download the script</code></a></p>

<h3>Script to mkceph:(in the dir &ldquo;mkceph&rdquo;)</h3>

<p>mkceph_with_pg.sh</p>

<h3>Script to produce workload:(all put in the dir &ldquo;run the test&rdquo;)</h3>

<p><b>run.sh</b>: aim to change the <code>ceph.conf</code> op thread number setting, then restart ceph, vm, and call <code>test.sh</code> to start the test. The test result will then be stored in the path &ldquo;/data/xcd/ceph-CephXCD/run_${id}&rdquo;</p>

<p><b>test.sh</b>: aim to clean run a vm_num loadline test, call <code>mhost-volume-test-xcd.sh</code> to run the actual test</p>

<p><b>mhost-volume-test-xcd.sh</b>: calls <code>clean_cache_on_ceph.sh</code>,<code>check_readahead.sh</code>, <code>volume-test_in_vm.sh</code>, <code>all.fio</code>, <code>volume-test_in_pc.sh</code>, <code>ceph_perf_counter_dump.sh</code>, <code>volume-test_in_ceph.sh</code>, <code>check_cephHealth.sh</code> to run the test(producing workload and collecting runtime system info &amp; ceph info), then collects all the data back to the Node1.
```console</p>

<h1>example:</h1>

<p>./mhost-volume-test-xcd.sh 1 $vm_num 3 4k &ldquo;vmNum_&rdquo;$vm_num ${qd} node13_net
```
tip: There is a VM list stored in file &ldquo;node13_net&rdquo;</p>

<h3>Script to parse the data:</h3>

<p><b>getLatbreakdown.sh</b>: aim to parse latency data from <code>*.asok</code> files using <code>post_perf.py</code></p>

<p><b>getResult.sh</b>: aim to parse iostat, mpstat data. It calls <code>post_volume_xcd.sh</code>, <code>post_osd.sh</code>, <code>post_cpu.sh</code>, <code>iostat.sh</code></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[AIO-STRESS vs. FIO]]></title>
    <link href="http://xuechendi.github.io/blog/2013/11/15/aio-stress-vs-fio/"/>
    <updated>2013-11-15T01:05:00+08:00</updated>
    <id>http://xuechendi.github.io/blog/2013/11/15/aio-stress-vs-fio</id>
    <content type="html"><![CDATA[<p>There is no big gap to me in producing workload by AIO-STREE and FIO.</p>

<p>Just because I haven&rsquo;t using much more advanced feature in FIO like replay trace, and I just using libaio as the driver.</p>

<p>But here, I just wanna show some interesting test to see some behavior by different setting of io_depth and io_batch. And <code>sadly</code> that is just some wrong setting I did before and produced some strange data then I just realized how these settings mean. T_T</p>

<h2>Just follow my lead to see how I found this wrong setting. ^_^</h2>

<p><img src="/images/AIO_vs_FIO/1.png" alt="findings" /></p>

<p>After a loadline test of io_depth setting, I just found the figures followed some rules, so using qemu logs, I draw above graph. This is a graph recording the io request intervals sending from qemu to librbd, and catched in librbd logs.</p>

<p>As we can see, when we set the queue_depth as 8, the first io request has quite small interval(0.075ms), but the interval between the 8th and 9th io request is pretty long(1.6ms). And if we set the queue_depth as 16, or 32. The burst interval happened just between the 16th to 17th, and 32th to 33th respectively.</p>

<p>So there comes the deduction:
Queue will send out all requests, then wait the last request completes . After that it starts to send out new queue.</p>

<h2>How to prove it?</h2>

<h3>Methodology:</h3>

<ol>
<li>. Set queue depth = 64</li>
<li>. make the 42th request sleep 3s in completion thread</li>
</ol>


<h3>&mdash;> If RBD waits 3s then receives the 65th requests, we can confirm the deduction.</h3>

<h3>How to do this?</h3>

<p>There is a function in librbd named C_AioRead::finish in AioCompletion.cc, so what we need to do is just randomly block this function for 3s, and see what happens.</p>

<p><img src="/images/AIO_vs_FIO/2.png" alt="proving" /></p>

<p>Aha! The results just show as what we expect.</p>

<h3>But there are still 3 possibilities can produce result like this.</h3>

<p>1.Wrong workload setting?
2.Ceph RBD design ?  if there is some lock in finish thread that blocks the submit thread
3.Virtio ?</p>

<p>The situation produced by FIO</p>

<p><img src="/images/AIO_vs_FIO/3.png" alt="proving" /></p>

<p>From this graph, we can see the result is really different by using aio-stress, which can help us to exclude last two possibilities. That means we must do some wrong setting in aio-stress or aio-stress just have some wierd strategy design.</p>

<p>Also we can see the difference in logs.</p>

<p><img src="/images/AIO_vs_FIO/4.png" alt="proving" /></p>

<p><code>Here I will add some aio-stress codes digging into soon, just not now~ Sorry</code></p>

<hr>


<p>The right setting for random io in aio-stress to act more meet our demand (Here I said random io is because this setting may result in no io merge in sequential io)</p>

<p><code>console
./aio-stress -O -o 3 -i 1 -d 64 -r4k -s 4m /dev/vdb
</code></p>

<p><img src="/images/AIO_vs_FIO/5.png" alt="proving" /></p>

<p><img src="/images/AIO_vs_FIO/6.png" alt="proving" /></p>
]]></content>
  </entry>
  
</feed>
