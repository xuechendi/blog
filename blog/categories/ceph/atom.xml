<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Ceph | XueChendi]]></title>
  <link href="http://xuechendi.github.io/blog/categories/ceph/atom.xml" rel="self"/>
  <link href="http://xuechendi.github.io/"/>
  <updated>2013-12-06T22:43:29+08:00</updated>
  <id>http://xuechendi.github.io/</id>
  <author>
    <name><![CDATA[Chendi.Xue]]></name>
    <email><![CDATA[xuechendi@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[How to do Ceph Latency Breakdown]]></title>
    <link href="http://xuechendi.github.io/blog/2013/12/05/how-to-do-ceph-latency-breakdown/"/>
    <updated>2013-12-05T10:38:00+08:00</updated>
    <id>http://xuechendi.github.io/blog/2013/12/05/how-to-do-ceph-latency-breakdown</id>
    <content type="html"><![CDATA[<h2>The Ceph Latency Breakdown process includes 3 steps:</h2>

<h3>1. Enable Log Configuration or Other performance count mechanism to record the runtime infomation.</h3>

<h3>2. Produce the workload and meanwhile dump the perfcounter</h3>

<h3>3. Parse the perfcounter dump data</h3>

<h2>Details:</h2>

<p>In ceph, a perfcounter mechanism has been adopted to do the latency breakdown, and the runtime info can be dumped by the admin_socket is an internal implementation in Ceph.</p>

<p>example:</p>

<p><1> codes
``` c git diff
diff &mdash;git a/src/osd/OSD.cc b/src/osd/OSD.cc
index d34bb9c..6c308d3 100644
&mdash;&ndash; a/src/osd/OSD.cc
+++ b/src/osd/OSD.cc
@@ -7055,10 +7055,10 @@ PGRef OSD::OpWQ::_dequeue()</p>

<p> void OSD::OpWQ::_process(PGRef pg, ThreadPool::TPHandle &amp;handle)
 {
&ndash;  utime_t pg_lock_lat = ceph_clock_now(g_ceph_context);
+  utime_t start = ceph_clock_now(g_ceph_context);
   pg->lock_suspend_timeout(handle);
&ndash;  pg_lock_lat = ceph_clock_now(g_ceph_context) &ndash; pg_lock_lat;
&ndash;  osd->logger->tinc(l_osd_pg_lock_lat, pg_lock_lat);
+  utime_t end = ceph_clock_now(g_ceph_context);
+  osd->logger->tinc(l_osd_pg_lock_lat, end &ndash; start);</p>

<p>   OpRequestRef op;
   {
```</p>

<p>Current Ceph Version only enabled admin_socket in Ceph Cluster Side, so we need to enable it in RBD side.</p>

<p><2> dump the data</p>

<p>``` bash ceph.conf</p>

<h1>Enable the admin_socket in QEMU RBD</h1>

<p>check ceph.conf
perf = false
&ndash;></p>

<h1>perf = false</h1>

<p>```</p>

<p>```xml QEMU disk.xml</p>

<pre><code>&lt;disk type='network' device='disk'&gt;
  &lt;driver name='qemu' type='raw' cache='none'/&gt;
  &lt;source protocol='rbd' name='xcd_8osd/volume-3:admin_socket=/var/run/ceph/node5_1.asok'/&gt;
  &lt;target dev='vdb' bus='virtio'/&gt;
  &lt;serial&gt;12f70341-d199-4fca-9270-56e5d6b80061&lt;/serial&gt;
  &lt;alias name='virtio-disk1'/&gt;
  &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/&gt;
&lt;/disk&gt;
</code></pre>

<p><code>
</code> console
virsh attach instance1 disk.xml</p>

<p>ceph &mdash;admin-daemon /var/run/ceph/${osd} perf dump >> ${osd}.txt
```</p>

<hr>


<h2>Below is the summary of all scripts:</h2>

<p><a href ="/downloads/Ceph_Latency_Breakdown_Script.zip"><code style="font-family: 'Fjalla One','Georgia','Helvetica Neue',Arial,sans-serif;font-weight:900;font-size:12px;">download the script</code></a></p>

<h3>Script to mkceph:(in the dir &ldquo;mkceph&rdquo;)</h3>

<p>mkceph_with_pg.sh</p>

<h3>Script to produce workload:(all put in the dir &ldquo;run the test&rdquo;)</h3>

<p><b>run.sh</b>: aim to change the <code>ceph.conf</code> op thread number setting, then restart ceph, vm, and call <code>test.sh</code> to start the test. The test result will then be stored in the path &ldquo;/data/xcd/ceph-CephXCD/run_${id}&rdquo;</p>

<p><b>test.sh</b>: aim to clean run a vm_num loadline test, call <code>mhost-volume-test-xcd.sh</code> to run the actual test</p>

<p><b>mhost-volume-test-xcd.sh</b>: calls <code>clean_cache_on_ceph.sh</code>,<code>check_readahead.sh</code>, <code>volume-test_in_vm.sh</code>, <code>all.fio</code>, <code>volume-test_in_pc.sh</code>, <code>ceph_perf_counter_dump.sh</code>, <code>volume-test_in_ceph.sh</code>, <code>check_cephHealth.sh</code> to run the test(producing workload and collecting runtime system info &amp; ceph info), then collects all the data back to the Node1.
```console</p>

<h1>example:</h1>

<p>./mhost-volume-test-xcd.sh 1 $vm_num 3 4k &ldquo;vmNum_&rdquo;$vm_num ${qd} node13_net
```
tip: There is a VM list stored in file &ldquo;node13_net&rdquo;</p>

<h3>Script to parse the data:</h3>

<p><b>getLatbreakdown.sh</b>: aim to parse latency data from <code>*.asok</code> files using <code>post_perf.py</code></p>

<p><b>getResult.sh</b>: aim to parse iostat, mpstat data. It calls <code>post_volume_xcd.sh</code>, <code>post_osd.sh</code>, <code>post_cpu.sh</code>, <code>iostat.sh</code></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[AIO-STRESS vs. FIO]]></title>
    <link href="http://xuechendi.github.io/blog/2013/11/15/aio-stress-vs-fio/"/>
    <updated>2013-11-15T01:05:00+08:00</updated>
    <id>http://xuechendi.github.io/blog/2013/11/15/aio-stress-vs-fio</id>
    <content type="html"><![CDATA[<p>There is no big gap to me in producing workload by AIO-STREE and FIO.</p>

<p>Just because I haven&rsquo;t using much more advanced feature in FIO like replay trace, and I just using libaio as the driver.</p>

<p>But here, I just wanna show some interesting test to see some behavior by different setting of io_depth and io_batch. And <code>sadly</code> that is just some wrong setting I did before and produced some strange data then I just realized how these settings mean. T_T</p>

<h2>Just follow my lead to see how I found this wrong setting. ^_^</h2>

<p><img src="/images/AIO_vs_FIO/1.png" alt="findings" /></p>

<p>After a loadline test of io_depth setting, I just found the figures followed some rules, so using qemu logs, I draw above graph. This is a graph recording the io request intervals sending from qemu to librbd, and catched in librbd logs.</p>

<p>As we can see, when we set the queue_depth as 8, the first io request has quite small interval(0.075ms), but the interval between the 8th and 9th io request is pretty long(1.6ms). And if we set the queue_depth as 16, or 32. The burst interval happened just between the 16th to 17th, and 32th to 33th respectively.</p>

<p>So there comes the deduction:
Queue will send out all requests, then wait the last request completes . After that it starts to send out new queue.</p>

<h2>How to prove it?</h2>

<h3>Methodology:</h3>

<ol>
<li>. Set queue depth = 64</li>
<li>. make the 42th request sleep 3s in completion thread</li>
</ol>


<h3>&mdash;> If RBD waits 3s then receives the 65th requests, we can confirm the deduction.</h3>

<h3>How to do this?</h3>

<p>There is a function in librbd named C_AioRead::finish in AioCompletion.cc, so what we need to do is just randomly block this function for 3s, and see what happens.</p>

<p><img src="/images/AIO_vs_FIO/2.png" alt="proving" /></p>

<p>Aha! The results just show as what we expect.</p>

<h3>But there are still 3 possibilities can produce result like this.</h3>

<p>1.Wrong workload setting?
2.Ceph RBD design ?  if there is some lock in finish thread that blocks the submit thread
3.Virtio ?</p>

<p>The situation produced by FIO</p>

<p><img src="/images/AIO_vs_FIO/3.png" alt="proving" /></p>

<p>From this graph, we can see the result is really different by using aio-stress, which can help us to exclude last two possibilities. That means we must do some wrong setting in aio-stress or aio-stress just have some wierd strategy design.</p>

<p>Also we can see the difference in logs.</p>

<p><img src="/images/AIO_vs_FIO/4.png" alt="proving" /></p>

<p><code>Here I will add some aio-stress codes digging into soon, just not now~ Sorry</code></p>

<hr>


<p>The right setting for random io in aio-stress to act more meet our demand (Here I said random io is because this setting may result in no io merge in sequential io)</p>

<p><code>console
./aio-stress -O -o 3 -i 1 -d 64 -r4k -s 4m /dev/vdb
</code></p>

<p><img src="/images/AIO_vs_FIO/5.png" alt="proving" /></p>

<p><img src="/images/AIO_vs_FIO/6.png" alt="proving" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to break into Ceph Codes]]></title>
    <link href="http://xuechendi.github.io/blog/2013/11/15/how-to-break-into-ceph-codes/"/>
    <updated>2013-11-15T01:00:00+08:00</updated>
    <id>http://xuechendi.github.io/blog/2013/11/15/how-to-break-into-ceph-codes</id>
    <content type="html"><![CDATA[<p>Working in Progress, will be finished by this weekend</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ObjectCacher: RBD Cache Codes]]></title>
    <link href="http://xuechendi.github.io/blog/2013/11/15/objectcacher-rbd-cache-codes/"/>
    <updated>2013-11-15T00:58:00+08:00</updated>
    <id>http://xuechendi.github.io/blog/2013/11/15/objectcacher-rbd-cache-codes</id>
    <content type="html"><![CDATA[<p>There is some cache mechanism in Ceph RBD.
And it is now in memory cache which supports write through and write back.</p>

<p>In this post, I would post what I have learnt from ObjectCacher codes.</p>

<h2>How to deploy RBD Cache</h2>

<p>there is a really clear instruction in ceph.com, just linked here <code><a href="http://ceph.com/docs/master/rbd/rbd-config-ref/?highlight=rbd%20cache">Ceph RBD Cache setting</a></code></p>

<h2>How to record log from RBD</h2>

<p>There is a little tricky to record log in RBD sides, because if you simply add the log settings in ceph.conf, you can only get logs when you do something using &ldquo;rbd&rdquo; command, like &ldquo;rbd lspools&rdquo;, etc. So simply adding log settings in ceph.conf(client side) can not help you to get logs from QEMU to librbd, the reason is unknown to me, but I just find a way to walk around this.</p>

<p>All you need to do is to add log setting in you instance.xml, then using libvirt to boot this instance or also you can just attach a new disk by using xml like below.</p>

<p>``` xml disk.xml</p>

<pre><code>&lt;disk type='network' device='disk'&gt;
  &lt;driver name='qemu' type='raw' cache='none'/&gt;
  &lt;source protocol='rbd' name='xcd_8osd/volume-3:debug_rbd=20:debug_objectcacher=20:log_file=/tmp/qemu-rbd.log'/&gt;
  &lt;target dev='vdb' bus='virtio'/&gt;
  &lt;serial&gt;12f70341-d199-4fca-9270-56e5d6b80061&lt;/serial&gt;
  &lt;alias name='virtio-disk1'/&gt;
  &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/&gt;
&lt;/disk&gt;
</code></pre>

<p>```</p>

<p>After attach this disk, you can find logs in /tmp/qemu-rbd.log or you can just write to qemu log.</p>

<p>``` xml write log to /var/libvirt/qemu/xxxxx(instance_name).log</p>

<pre><code>  &lt;source protocol='rbd' name='xcd_8osd/volume-3:debug_rbd=20:debug_objectcacher=20:log_to_stderr=true'/&gt;
</code></pre>

<p>```
There is a really clear codes path we can find from logs, so just using log to debug or hack into the rbd codes.</p>

<h2>Dig into the codes</h2>

<p>If we enable rbd cache in ceph.conf(client side), the ObjectCacher object is created when this rbd image attach to the vm.
Here is the overview.</p>

<p><img src="/images/RBD_Cache/overview.png" width=60%></p>

<p>From above we can see each RBD has a object to store all information name ImageCtx(image context), so there is only one ObjectCacher in one RBD cache and also different RBD images can not share their cache till now.</p>

<p>ObjectCacher uses poolid, oid(objectid), then offset and length to index cache in memory.</p>

<p>Then here is a graph to show how ceph using ObjectCacher.</p>

<p><img src="/images/RBD_Cache/workflow_send_req.png" width=100%></p>

<p><img src="/images/RBD_Cache/workflow_recv_req.png" width=50%></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Study of Data distribution methods--Crush]]></title>
    <link href="http://xuechendi.github.io/blog/2013/11/15/crush-ceph-object-distribution-method/"/>
    <updated>2013-11-15T00:51:00+08:00</updated>
    <id>http://xuechendi.github.io/blog/2013/11/15/crush-ceph-object-distribution-method</id>
    <content type="html"><![CDATA[<p>wanna learn, mark here</p>
]]></content>
  </entry>
  
</feed>
