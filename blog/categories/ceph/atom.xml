<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Ceph | XueChendi]]></title>
  <link href="http://xuechendi.github.io/blog/categories/ceph/atom.xml" rel="self"/>
  <link href="http://xuechendi.github.io/"/>
  <updated>2015-06-09T03:26:31+08:00</updated>
  <id>http://xuechendi.github.io/</id>
  <author>
    <name><![CDATA[Chendi.Xue]]></name>
    <email><![CDATA[xuechendi@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Attach RBD volumes on CentOS]]></title>
    <link href="http://xuechendi.github.io/blog/2014/09/15/attach-rbd-volumes-on-centos/"/>
    <updated>2014-09-15T02:23:00+08:00</updated>
    <id>http://xuechendi.github.io/blog/2014/09/15/attach-rbd-volumes-on-centos</id>
    <content type="html"><![CDATA[<p>Last Friday, I was pretty struggling in attaching rbd volume by CentOS libvirt, so just wanna to record the right steps and also talk about my may of troubleshooting</p>

<h3>1. Check your qemu-kvm supported-format</h3>

<h3>2. Attach volume when auth_supported = cephx</h3>

<h3>3. Trouble shooting suggestions</h3>

<h3>1. Check your qemu-kvm supported-format</h3>

<p>When I firsted encoutered the error of unable to attach rbd volume, error is looks like below
<code>console error
[root@client03 vdbs]# virsh attach-device vclient01 vclient01.xml
error: Failed to attach device from vclient01.xml
error: internal error unable to execute QEMU command '__com.redhat_drive_add': Device 'drive-virtio-disk1' could not be initialized
</code></p>

<p>First thought pops up is &ldquo;qemu driver?&rdquo;
So how to check if my libvirt version can support rbd?</p>

<ol>
<li>create a vm without rbd volume and lsof the vm pid to see which qemu-kvm it is calling( I found when using ubuntu , it calls kvm not qemu-kvm )
```
[root@client03]virsh create vclient.xml
Domain vclient created from vclient.xml</li>
</ol>


<p>[root@client03]pid=<code>ps aux | grep vdlient | awk '{print $2}'</code>
[root@client03]lsof -p $pid
[root@client03 vdbs]# lsof -p 125681 | grep kvm
qemu-kvm 125681 qemu  cwd    DIR              253,0       4096      2 /
qemu-kvm 125681 qemu  rtd    DIR              253,0       4096      2 /
qemu-kvm 125681 qemu  txt    REG              253,0   52636261   5820 /usr/libexec/qemu-kvm
```</p>

<p>Then you know which qemu-kvm it is calling, and just check the supported format
<code>
[root@client03]# /usr/libexec/qemu-kvm --drive format=?
Supported formats: raw cow qcow vdi vmdk cloop dmg bochs vpc vvfat qcow2 qed vhdx parallels nbd blkdebug host_cdrom host_floppy host_device file gluster gluster gluster gluster rbd
</code></p>

<p>OK, my qemu-kvm can support rbd, which is great</p>

<p>If your qemu-kvm can not support ceph, you can download a ceph patched qemu-kvm from here
<a href="http://ceph.com/packages/ceph-extras/rpm/">http://ceph.com/packages/ceph-extras/rpm/</a></p>

<p>```</p>

<h1>first checking your own qemu version</h1>

<p>[root@client03]# rpm -qa | grep qemu
qemu-img-0.12.1.2-2.415.el6.x86_64
qemu-kvm-0.12.1.2-2.415.el6.x86_64</p>

<h1>then download the corresponding ceph patched version and install</h1>

<p>[root@client03]# rpm &mdash;oldpackage -Uvh qemu-kvm-0.12.1.2-2.415.el6.3ceph.x86_64 qemu-img-0.12.1.2-2.415.el6.3ceph.x86_64</p>

<h1>check</h1>

<p>[root@client03 vdbs]# rpm -qa | grep qemu
qemu-img-0.12.1.2-2.415.el6.3ceph.x86_64
qemu-kvm-0.12.1.2-2.415.el6.3ceph.x86_64
```</p>

<h3>2. Attach volumes when auth_supported = cephx</h3>

<p>After upgrading qemu to support ceph, I still get the same error, a friend of mine suggested me to add my rbd disk xml into vm xml, so I can get some other debug logs
<code>
[root@client03 ~]# virsh create /opt/vmxml/vclient01.xml
error: Failed to create domain from /opt/vmxml/vclient01.xml
error: internal error Process exited while reading console log output: char device redirected to /dev/pts/2
qemu-kvm: -drive file=rbd:rbd/volume-1:auth_supported=none,if=none,id=drive-virtio-disk1,format=raw,serial=009ad738-1a2e-4d9c-bf22-1993c8c67ade: error connecting
qemu-kvm: -drive file=rbd:rbd/volume-1:auth_supported=none,if=none,id=drive-virtio-disk1,format=raw,serial=009ad738-1a2e-4d9c-bf22-1993c8c67ade: could not open disk image rbd:rbd/volume-1:auth_supported=none: Operation not supported
</code></p>

<p>Here is it, it says I can not open disk image, why there is some auth_supported setting tails my rbd volume, I did not set this in my vm xml
So it&rsquo;s easy to tell when I set no auth in libvirt, it will defaultly tail &lsquo;auth_supported=none&rsquo;. That is what causing me unable to open the rbd image.</p>

<p>What to do? Add the auth to libvirt and disk.xml
```
[root@client03 ~]# ceph auth get-or-create client.admin mon &lsquo;allow <em>&rsquo; osd &lsquo;allow </em>&rsquo; -o /etc/ceph/ceph.client.admin.keyring
[root@client03 ~]# cat /etc/ceph/ceph.client.admin.keyring
[client.admin]</p>

<pre><code>key = AQDzMfRTOA1kMxAAtkzqOeuamZP/Bl8IYrf4ug==
</code></pre>

<p>[root@client03 ~]# ceph_cluster_uuid=<code>ceph -s | grep cluster | awk '{print $2}'</code>
[root@client03 ~]# cat > secret.xml &lt;&lt;EOF</p>

<blockquote><p><secret ephemeral='no' private='no'>
  <uuid>$ceph_cluster_uuid</uuid>
  <usage type='ceph'></p>

<pre><code>&lt;name&gt;client.admin secret&lt;/name&gt;
</code></pre>

<p>  </usage>
</secret>
EOF
[root@client03 ~]# virsh secret-define &mdash;file secret.xml
Secret b535f448-46f5-4350-b5d9-1b648b5a09e3 created
[root@client03 ~]# virsh secret-set-value b535f448-46f5-4350-b5d9-1b648b5a09e3 AQDzMfRTOA1kMxAAtkzqOeuamZP/Bl8IYrf4ug==
Secret value set</p></blockquote>

<p>[root@client03 vdbs]# cat vclient01.xml
<disk type='network' device='disk'></p>

<pre><code>&lt;driver name='qemu' type='raw'/&gt;
&lt;auth username='admin'&gt;
    &lt;secret type='ceph' uuid='b535f448-46f5-4350-b5d9-1b648b5a09e3'/&gt;
&lt;/auth&gt;
&lt;source protocol='rbd' name='rbd/volume-1' /&gt;
&lt;target dev='vdb' bus='virtio'/&gt;
&lt;serial&gt;009ad738-1a2e-4d9c-bf22-1993c8c67ade&lt;/serial&gt;
&lt;address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/&gt;
</code></pre>

<p></disk>
```</p>

<p>Now you can attach rbd volume successfully</p>

<h3>3. Trouble shooting suggestions</h3>

<p>For libvirt is like a wrapper, and it calls qemu-kvm in RH distribution and kvm in UBUNTU/DEBIAN distribution, when you failed in using libvirt to create vm,  you can try to use qemu-kvm
It&rsquo;s like libvirt will translate the vm xml into qemu-kvm command, like in my settup, the qemu-kvm calls
<code>
/usr/libexec/qemu-kvm -name vclient01 -S -M rhel6.5.0 -cpu SandyBridge,+erms,+smep,+fsgsbase,+pdpe1gb,+rdrand,+f16c,+osxsave,+dca,+pcid,+pdcm,+xtpr,+tm2,+est,+smx,+vmx,+ds_cpl,+monitor,+dtes64,+pbe,+tm,+ht,+ss,+acpi,+ds,+vme -enable-kvm -m 512 -realtime mlock=off -smp 1,sockets=1,cores=1,threads=1 -uuid d2ae09f2-8b6e-7781-acb3-2dfa1ef18e13 -nodefconfig -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/vclient01.monitor,server,nowait -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc,driftfix=slew -no-kvm-pit-reinjection -no-shutdown -device piix3-usb-uhci,id=usb,bus=pci.0,addr=0x1.0x2 -drive file=/opt/image/vclient01.img,if=none,id=drive-virtio-disk0,format=raw,cache=none -device virtio-blk-pci,scsi=off,bus=pci.0,addr=0x4,drive=drive-virtio-disk0,id=virtio-disk0,bootindex=1 -drive file=rbd:rbd/volume-1,if=none,id=drive-virtio-disk1,format=raw,serial=009ad738-1a2e-4d9c-bf22-1993c8c67ade -device virtio-blk-pci,scsi=off,bus=pci.0,addr=0x6,drive=drive-virtio-disk1,id=virtio-disk1 -chardev pty,id=charserial0 -device isa-serial,chardev=charserial0,id=serial0 -device usb-tablet,id=input0 -vnc 0.0.0.0:0 -k en-us -vga cirrus -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x5
</code>
So using qemu-kvm, you can do some parameter changes to test and troubleshoot</p>

<p>OK, THAT&rsquo;S ALL</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Dive into Ceph Write IO Path]]></title>
    <link href="http://xuechendi.github.io/blog/2013/12/18/dive-into-ceph-write-io-path/"/>
    <updated>2013-12-18T14:41:00+08:00</updated>
    <id>http://xuechendi.github.io/blog/2013/12/18/dive-into-ceph-write-io-path</id>
    <content type="html"><![CDATA[<p><em>After talking with a friend of mine who is really intelligent and awesome and intelligent, just thinking about to post this article to get through the Ceph OSD Write IO path. There I am sorry to say must exists some misunderstanding in this article, but I hope it will still means something for people who read this article&hellip;</em></p>

<p><em>Ok, start here.</em></p>

<h6>tip: I am not planning finish this article in a fast way, for I am trying to make it more meaninhful.</h6>

<p>======</p>

<h2>Content</h2>

<ul>
<li>Ceph Write IO Path overview</li>
<li>3 phase codes deep dive</li>
<li>Throttle and lock</li>
<li>Latency breakdown</li>
</ul>


<h2>Referrence:</h2>

<ul>
<li><a href="https://ceph.com/docs/master/dev/object-store/">OBJECT STORE ARCHITECTURE OVERVIEW</a></li>
<li><a href="https://ceph.com/docs/master/dev/osd_internals/map_message_handling/">MAP AND PG MESSAGE HANDLING</a></li>
<li><a href="http://ceph.com/docs/master/dev/osd_internals/osd_throttles">OSD Internals</a></li>
<li><a href="https://ceph.com/docs/master/dev/osd_internals/osd_overview/">OSD</a></li>
</ul>


<p>There is a IO path graph on ceph.com, pls click <code><a href="http://ceph.com/docs/master/dev/osd_internals/osd_throttles/?highlight=wbthrottle">OSD Internals</a></code>, I find the graph is a bit difficult to see, so redraw it in a vertical way</p>

<div style="float:left;width:100%">
<div style="float:left; width:58%;">
    <div style="border-left:red solid 5px; padding-left:5%; ">
    <h3>Queue and thread overview</h3>
        <h5>Will finish today</h5>
    </div>
</div>

<img src="/images/ceph_write_path/io_path_1.png" style="margin-left:5%; width:35%; border:5px solid #222; float:left;">
</div>


<h3>Below I will describe the write io path by dividing into 3 parts, Dispatch write io msg, write to journal, write to filestore :</h3>

<h3>DispatchThread</h3>

<div style="font-size:12px;  border: solid #999 1px; background-color: #eee; width:100%; overflow: scroll; padding:5px; white-space:nowrap;text-overflow:scroll;">
1:2013-12-25 15:20:27.824912 7f8fda903700 20 <red>osd.12</red> 344 </red>_dispatch</red> 0x55803b40 <red>osd_op</red>(client.6219.0:8 rbd_data.10126b8b4567.0000000000000000 [write 0~512] 3.ff00f0a2 e344) v4<br>
2:2013-12-25 15:20:27.824935 7f8fda903700 15 <red>osd.12</red> 344 <red>enqueue_op</red> 0x54e7cb60 prio 63 cost 512 latency 0.000163 <red>osd_op</red>(client.6219.0:8 rbd_data.10126b8b4567.0000000000000000 [write 0~512] 3.ff00f0a2 e344) v4<br>
</div>


<p><code>c DispatchThread
qitem = mqueue.dequeue() //mqueue is enqueued by Pipe::Reader, it stores every msg received by this osd.
m = qitem.get_message()
msgr→ms_deliver_dispatch(m)
p = dispatchers.begin()
p→ms_dispatch(m)  =&gt; OSD::ms_dispatch(m) // find a dispatcher can handle this msg
_dispatch(m) // OSD.cc, log above is printed in this function
dispatch_op(op) // switch the MSG type in this function, and in this case, msg-&gt;type would be "CEPH_MSG_OSD_OP"
handle_op(op) //do some preparation including refresh the osd map
enqueue_op(pg, op)
op_wq.queue(pg, op) //When OSD daemon starts, an OSD instance will be created, here calls the osd-&gt;op_wq-&gt;_enqueue(pair&lt;...&gt;) to put the op into osd-&gt;op_wq-&gt;pqueue
</code></p>

<h3>OSD:OpWQ</h3>

<div style="font-size:12px;  border: solid #999 1px; background-color: #eee; width:100%; overflow: scroll; padding:5px; white-space:nowrap;text-overflow:scroll;">
3:2013-12-25 15:20:27.824972 7f8fc70dc700 12 OSD::op_tp worker wq <red>OSD::OpWQ</red> <red>start processing</red> 0x1 (1 active)<br>
4:2013-12-25 15:20:27.824989 7f8fc70dc700 10 <red>osd.12</red> 344 <red>dequeue_op</red> 0x54e7cb60 prio 63 cost 512 latency 0.000217 osd_op(client.6219.0:8 rbd_data.10126b8b4567.0000000000000000 <red>[write 0~512]</red> 3.ff00f0a2 e344) v4 pg pg[3.a2( v 344'3679 (18'678,344'3679] local-les=338 n=3 ec=15 les/c 338/342 337/337/334) [12,13] r=0 lpr=337 mlcod 344'3678 active+clean]<br>
5:2013-12-25 15:20:27.825069 7f8fc70dc700 10 <red>osd.12</red> pg_epoch: 344 pg[3.a2( v 344'3679 (18'678,344'3679] local-les=338 n=3 ec=15 les/c 338/342 337/337/334) [12,13] r=0 lpr=337 mlcod 344'3678 active+clean] <red>do_op</red> osd_op(client.6219.0:8 rbd_data.10126b8b4567.0000000000000000 <red>[write 0~512]</red> 3.ff00f0a2 e344) v4 <red>may_write -> write-ordered</red><br>
7:2013-12-25 15:20:27.825284 7f8fc70dc700 10 <red>osd.12</red> pg_epoch: 344 pg[3.a2( v 344'3679 (18'678,344'3679] local-les=338 n=3 ec=15 les/c 338/342 337/337/334) [12,13] r=0 lpr=337 mlcod 344'3678 active+clean] <red>execute_ctx</red> 0x5548d800<br>
8:2013-12-25 15:20:27.825302 7f8fc70dc700 10 <red>osd.12</red> pg_epoch: 344 pg[3.a2( v 344'3679 (18'678,344'3679] local-les=338 n=3 ec=15 les/c 338/342 337/337/334) [12,13] r=0 lpr=337 mlcod 344'3678 active+clean] <red>do_op</red> ff00f0a2/rbd_data.10126b8b4567.0000000000000000/head//3 [write 0~512] ov 344'3679 av 344'3680 snapc 0=[] snapset 0=[]:[]+head<br>
9:2013-12-25 15:20:27.825322 7f8fc70dc700 10 <red>osd.12</red> pg_epoch: 344 pg[3.a2( v 344'3679 (18'678,344'3679] local-les=338 n=3 ec=15 les/c 338/342 337/337/334) [12,13] r=0 lpr=337 mlcod 344'3678 active+clean] <red>do_osd_op</red> ff00f0a2/rbd_data.10126b8b4567.0000000000000000/head//3 [write 0~512]<br>
10:2013-12-25 15:20:27.825338 7f8fc70dc700 10 <red>osd.12</red> pg_epoch: 344 pg[3.a2( v 344'3679 (18'678,344'3679] local-les=338 n=3 ec=15 les/c 338/342 337/337/334) [12,13] r=0 lpr=337 mlcod 344'3678 active+clean] <red>do_osd_op</red>  write 0~512<br>
11:2013-12-25 15:20:27.825361 7f8fc70dc700 15 <red>osd.12</red> pg_epoch: 344 pg[3.a2( v 344'3679 (18'678,344'3679] local-les=338 n=3 ec=15 les/c 338/342 337/337/334) [12,13] r=0 lpr=337 mlcod 344'3678 active+clean] <red>do_osd_op_effects</red> on session 0x54fb7d20<br>
12:2013-12-25 15:20:27.825653 7f8fc70dc700 10 <red>osd.12</red> pg_epoch: 344 pg[3.a2( v 344'3680 (18'679,344'3680] local-les=338 n=3 ec=15 les/c 338/342 337/337/334) [12,13] r=0 lpr=337 luod=344'3679 lcod 344'3679 mlcod 344'3678 active+clean] <red>new_repop</red> rep_tid 24 on osd_op(client.6219.0:8 rbd_data.10126b8b4567.0000000000000000 [write 0~512] 3.ff00f0a2 e344) v4<br>
13:2013-12-25 15:20:27.825678 7f8fc70dc700  7 <red>osd.12</red> pg_epoch: 344 pg[3.a2( v 344'3680 (18'679,344'3680] local-les=338 n=3 ec=15 les/c 338/342 337/337/334) [12,13] r=0 lpr=337 luod=344'3679 lcod 344'3679 mlcod 344'3678 active+clean] <red>issue_repop</red> rep_tid 24 o ff00f0a2/rbd_data.10126b8b4567.0000000000000000/head//3<br>
14:2013-12-25 15:20:27.825724 7f8fc70dc700 10 <red>osd.12</red> pg_epoch: 344 pg[3.a2( v 344'3680 (18'679,344'3680] local-les=338 n=3 ec=15 les/c 338/342 337/337/334) [12,13] r=0 lpr=337 luod=344'3679 lcod 344'3679 mlcod 344'3678 active+clean] <red>eval_repop</red> repgather(0x55842780 344'3680 rep_tid=24 wfack=12,13 wfdisk=12,13 op=osd_op(client.6219.0:8 rbd_data.10126b8b4567.0000000000000000 [write 0~512] 3.ff00f0a2 e344) v4) wants=ad<br>
15:2013-12-25 15:20:27.825759 7f8fc70dc700 10 <red>osd.12</red> pg_epoch: 344 pg[3.a2( v 344'3680 (18'679,344'3680] local-les=338 n=3 ec=15 les/c 338/342 337/337/334) [12,13] r=0 lpr=337 luod=344'3679 lcod 344'3679 mlcod 344'3678 active+clean] <red>apply_repop</red>  applying update on repgather(0x55842780 344'3680 rep_tid=24 wfack=12,13 wfdisk=12,13 op=osd_op(client.6219.0:8 rbd_data.10126b8b4567.0000000000000000 [write 0~512] 3.ff00f0a2 e344) v4)<br>
16:2013-12-25 15:20:27.825784 7f8fc70dc700  5 filestore(/data/osd.12) <red>queue_transactions</red> existing osr(3.a2 0x18b7840)/0x18b7840<br>
17:2013-12-25 15:20:27.825802 7f8fc70dc700  5 filestore(/data/osd.12) <red>queue_transactions</red> (writeahead) 3457004 0x5548dbf0,0x5548db38<br>
18:2013-12-25 15:20:27.825807 7f8fc70dc700 10 journal <red>op_journal_transactions</red> 3457004 0x5548dbf0,0x5548db38<br>
19:2013-12-25 15:20:27.825840 7f8fc70dc700 10 <red>osd.12</red> 344 <red>dequeue_op</red> 0x54e7cb60 finish<br>
20:2013-12-25 15:20:27.825843 7f8fe003b700 20 journal <red>write_thread_entry</red> <red>woke up</red><br>
21:2013-12-25 15:20:27.825846 7f8fc70dc700 15 OSD::op_tp worker wq <red>OSD::OpWQ</red> <red>done processing</red> 0x1 (0 active)<br>
</div>


<p>```c OSD:OpWQ
/<em>ThreadPool::worker is a thread looping all the time to dequeue and process items of workerqueue in his threadpool, codes in WorkQueue.cc</em>/
wq = work_queues[last_work_queue]; //wq is osd->op_wq here
void <em>item = wq-><em>void_dequeue(); //osd->op_wq-></em>dequeue() dequeue one item from osd->op_wq->pqueue, and push into pg_for_processing[&amp;</em>pg]
wq-><em>void_process(item, tp_handle); //osd->op_wq-></em>process(pg) pop item from pg_for_processing[&amp;*pg]
osd→dequeue_op(pg, op); //log above taged by &ldquo;dequeue_op&rdquo; is printed here. it calls do_request function, then print another &ldquo;dequeue_op&rdquo; msg.
pg→do_request(op); //implemented in ReplicatedPG.cc, switch the op type and calls do_op;
do_op(op); // Initiate the OpContext ctx, then calls execute_ctx(ctx);</p>

<p>execute_ctx(ctx);</p>

<pre><code>prepare_transaction(ctx);
    do_osd_ops(ctx, ctx-&gt;ops); //switch (op.op), if read, call osd-&gt;store-&gt;read; if write, call ObjectStore::Transaction::write() prepare the transaction.
ctx-&gt;reply = new MOSDOpReply(m, 0, get_osdmap()-&gt;get_epoch(), 0); //Write operations aren't allowed to return a data payload
RepGather *repop = new_repop(ctx, obc, rep_tid); //replica operation
issue_repop(repop, now); //add itself and all the replica peer into repop-&gt;wairfor_disk and repop-&gt;waitfor_ack; 
    for peer in itsself and its peers:
        repop-&gt;waitfor_ack.insert(peer);
        repop-&gt;waitfor_disk.insert(peer);
    MOSDSubOp *wr = new MOSDSubOp(...)
    osd-&gt;send_message_osd_cluster(peer, wr, get_osdmap()-&gt;get_epoch()); //send supop to its peers
eval_repop(repop);
    apply_repop(repop); //if the writing to itself hasn't been applied, this function will put the transactions into osd-&gt;store-&gt;journal-&gt;writeq(talks about this later)
    if(repop-&gt;waitfor_disk.empty()) send commit msg
    if(repop-&gt;waitfor_ack.empty()) send ack msg
    if(repop-&gt;waitfor_ack.empty() &amp;&amp; repop-&gt;waitfor_disk.empty() &amp;&amp; repop-&gt;applied) remove(repop) 
</code></pre>

<p>//then back to discuss the apply_repop function.
apply_repop(repop)</p>

<pre><code>//Set the completion action will be taken when this operation finishes here.
Context *oncommit = new C_OSD_OpCommit(this, repop);
Context *onapplied = new C_OSD_OpApplied(this, repop);
Context *onapplied_sync = new C_OSD_OndiskWriteUnlock(...);
osd→store→queue_transactions(…) //apply_repop calls queue_transactions function, and because when osd instance first initiated, store is created as a FileStore instance in funtion OSD::pre_init(), so here, we actually calls the FileStore::queue_transactions();
</code></pre>

<p>FileStore::queue_transactions()</p>

<pre><code>osr-&gt;queue_journal(o-&gt;op);
_op_journal_transactions(o-&gt;tls, o-&gt;op,new C_JournaledAhead(this, osr, o, ondisk),osd_op); //C_JournaledAhead::finish() will be called when this transaction finishes.
</code></pre>

<p>_op_journal_transactions(&hellip;)
journal->submit_entry(op, tbl, data_align, onjournal, osd_op); //journal is osd->store->journal here</p>

<pre><code>completions.push_back(completion_item(...)); //a completions queue, store the completion instance (which tells what action should be taken when this writing finished).
writeq.push_back(write_item(seq, e, alignment, osd_op)); //put the osd_op into osd-&gt;store-&gt;journal-&gt;writeq
writeq_cond.Signal(); //wake up here writeq_cond.Wait() is =&gt; FileJournal::write_thread_entry()
</code></pre>

<p>osd-><em>process_finish() //ThreadPool::worker </em>void_process_finish , this function mainly erase the in_use info of pgs.</p>

<p>//Ok, the worker_queue(osd->op_wq) finishes a whole loop till now, and back to get a new request.<br/>
```</p>

<h3>FileJournal</h3>

<div style="font-size:12px;  border: solid #999 1px; background-color: #eee; width:100%; overflow: scroll; padding:5px; white-space:nowrap;text-overflow:scroll;">
20:2013-12-25 15:20:27.825843 7f8fe003b700 20 journal <red>write_thread_entry</red> woke up<br>
22:2013-12-25 15:20:27.825856 7f8fe003b700 15 journal <red>prepare_single_write</red> 1 will write 10288709632 : seq 3457004 len 2577 -> 4096 (head 40 pre_pad 0 ebl 2577 post_pad 1439 tail 40) (ebl alignment -1)<br>
23:2013-12-25 15:20:27.825882 7f8fe003b700 15 journal <red>do_aio_write</red> writing 10288709632~4096 + header<br>
24:2013-12-25 15:20:27.825884 7f8fe003b700 20 journal <red>write_aio_bl</red> 0~4096 seq 0<br>
25:2013-12-25 15:20:27.825887 7f8fe003b700 20 journal <red>write_aio_bl</red> .. 0~4096 in 1<br>
26:2013-12-25 15:20:27.825940 7f8fe003b700 20 journal <red>write_aio_bl</red> 10288709632~4096 seq 3457004<br>
27:2013-12-25 15:20:27.825944 7f8fe003b700 20 journal <red>write_aio_bl</red> .. 10288709632~4096 in 1<br>
28:2013-12-25 15:20:27.826359 7f8fe003b700 20 journal <red>write_thread_entry</red> going to sleep<br>
29:2013-12-25 15:20:27.826340 7f8fdf83a700 20 journal <red>write_finish_thread_entry</red> waiting for aio(s)<br>
30:2013-12-25 15:20:27.826365 7f8fdf83a700 10 journal <red>write_finish_thread_entry</red> aio 0~4096 done<br>
31:2013-12-25 15:20:27.826369 7f8fdf83a700 20 journal <red>check_aio_completion</red><br>
32:2013-12-25 15:20:27.826371 7f8fdf83a700 20 journal <red>check_aio_completion</red> completed seq 0 0~4096<br>
33:2013-12-25 15:20:27.826376 7f8fdf83a700 20 journal <red>write_finish_thread_entry</red> waiting for aio(s)<br>
34:2013-12-25 15:20:27.826800 7f8fdf83a700 10 journal <red>write_finish_thread_entry</red> aio 10288709632~4096 done<br>
35:2013-12-25 15:20:27.826806 7f8fdf83a700 20 journal <red>check_aio_completion</red><br>
36:2013-12-25 15:20:27.826807 7f8fdf83a700 20 journal <red>check_aio_completion</red> completed seq 3457004 10288709632~4096<br>
37:2013-12-25 15:20:27.826811 7f8fdf83a700 20 journal <red>check_aio_completion</red> queueing finishers through seq 3457004<br>
38:2013-12-25 15:20:27.826813 7f8fdf83a700 10 journal <red>queue_completions_thru</red> seq 3457004 queueing seq 3457004 0x55df22a0 lat 0.000987<br>
39:2013-12-25 15:20:27.826832 7f8fdf83a700 20 journal <red>write_finish_thread_entry</red> sleeping<br>
40:2013-12-25 15:20:27.826841 7f8fdf039700 10 finisher(0x18e7018) <red>finisher_thread</red> doing [0x55df22a0]<br>
41:2013-12-25 15:20:27.826849 7f8fdf039700  5 filestore(/data/osd.12) <red>_journaled_ahead</red> 0x54ff4370 seq 3457004 osr(3.a2 0x18b7840) 0x5548dbf0,0x5548db38<br>
42:2013-12-25 15:20:27.826856 7f8fdf039700  5 filestore(/data/osd.12) <red>queue_op</red> 0x54ff4370 seq 3457004 osr(3.a2 0x18b7840) 2565 bytes   (queue has 1 ops and 2565 bytes)<br>
43:2013-12-25 15:20:27.826869 7f8fdf039700 10 finisher(0x18e7018) <red>finisher_thread</red> done with [0x55df22a0]<br>
44:2013-12-25 15:20:27.826873 7f8fdf039700 10 finisher(0x18e7018) <red>finisher_thread</red> empty<br>
45:2013-12-25 15:20:27.826875 7f8fdf039700 10 finisher(0x18e7018) <red>finisher_thread</red> sleeping<br>
</div>


<p>```c FileJournal::Writer
FileJournal::write_thread_entry(); //with/without aio will be different here.</p>

<pre><code>r = prepare_multi_write(bl, orig_ops, orig_bytes); //r = prepare_single_write(bl, queue_pos, orig_ops, orig_bytes); pop item from writeq and push to journalq, journalq is to be used when need to sync.
if (aio) write_aio_bl(pos, second, writing_seq);
//if using aio, then will be like below
    io_prep_pwritev(&amp;aio.iocb, fd, aio.iov, n, pos);
    io_submit(aio_ctx, 1, &amp;piocb);
    write_finish_cond.Signal(); //write_finish_thread will wake up here
    /*==============================*/
    write_finish_thread_entry();
        check_aio_completion(); //check each aio_info in the aio_queue(we push each write io into aio_queue in write_aio_bl function)
        queue_completions_thru(journaled_seq); 
    /*==============================*/
    else do_write(bl);
//if not using aio, will be like below
    writev(fd, start, num);
    fdatasync(fd);
    queue_completions_thru(journaled_seq);
</code></pre>

<p>queue_completions_thru(journaled_seq);</p>

<pre><code>completion_item next = completion_peek_front(); //get item from completions
finisher-&gt;queue(next.finish); put the completion.finish =&gt; C_JournaledAhead::finish() in finisher thread
finisher_cond.Signal(); //wake up osd-&gt;store-&gt;journal-&gt;finisher thread, then calls fs-&gt;_journaled_ahead(osr, o, ondisk);
</code></pre>

<p>FileStore::_journaled_ahead(OpSequencer <em>osr, Op </em>o, Context <em>ondisk)
FileStore::queue_op(OpSequencer </em>osr, Op *o)
osd->store->op_wq.queue(osr); //Notice here, it calls the osd->store->op_wq.queue not osd->op_wq, and actually the io unit enqueue the osd->store->op_queue
ondisk_finisher.queue(ondisk); //wake up the Finisher::finisher_thread_entry() which calls C_OSD_OpCommit.finish() => pg->op_commit(repop);</p>

<pre><code>repop-&gt;waitfor_disk.erase(whoami);
repop-&gt;waitfor_ack.erase(whoami);
eval_repop(repop); //check if this rep_op can send its commit and ack
</code></pre>

<p>//Ok, Till now, Journal writing process is finished, the filestore write just begins.
//Another thing is that, if we use btrfs, then in function queue_transactions, the write to journal and filestore will be parallel.
```</p>

<p>FileStore::OpWQ</p>

<div style="font-size:12px;  border: solid #999 1px; background-color: #eee; width:100%; overflow: scroll; padding:5px; white-space:nowrap;text-overflow:scroll;">

</div>


<p>```c FileStore::OpWQ
store-><em>do_op(osr, handle); //ThreadPool::worker wq-></em>void_process(item, tp_handle);
<em>do_transactions(o->tls, o->op, &amp;handle);
</em>do_transaction(**(o->tls->begin), op_seq, trans_num, handle);</p>

<pre><code>switch(op) case Transaction::OP_WRITE: 
    decode cid, oid, off, len, bl from osr-&gt;q-&gt;tls
    r = _write(cid, oid, off, len, bl, replica);
        FileStore::_write(...) =&gt; writev
</code></pre>

<p>store-><em>do_op(osr, handle); //ThreadPool::worker wq-></em>void_process_finish(item);
store->_finish_op(osr)</p>

<pre><code>if (o-&gt;onreadable_sync) o-&gt;onreadable_sync-&gt;complete(0); //C_OSD_OndiskWriteUnlock.finish()
op_finisher.queue(o-&gt;onreadable); // wake up the Finisher::finisher_thread_entry() and C_OSD_OpApplied.finish() =&gt; pg-&gt;op_applied(repop);
</code></pre>

<p>pg->op_applied(repop)</p>

<pre><code>repop-&gt;applying = false;
repop-&gt;applied = true;
repop-&gt;waitfor_ack.erase(whoami);
eval_repop(repop); //check if this rep_op can send its commit and ack
</code></pre>

<p>```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ceph Best known Performance Collection]]></title>
    <link href="http://xuechendi.github.io/blog/2013/12/13/ceph-best-known-performance-collection/"/>
    <updated>2013-12-13T15:35:00+08:00</updated>
    <id>http://xuechendi.github.io/blog/2013/12/13/ceph-best-known-performance-collection</id>
    <content type="html"><![CDATA[<p>In this posting, I hope to collect  <em>TILL NOW BEST KNOWN</em>  Ceph Performance and also their deployment/setup method.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Building Ceph]]></title>
    <link href="http://xuechendi.github.io/blog/2013/12/12/building-ceph/"/>
    <updated>2013-12-12T11:17:00+08:00</updated>
    <id>http://xuechendi.github.io/blog/2013/12/12/building-ceph</id>
    <content type="html"><![CDATA[<p>Actually, I had bulid ceph from codes for many times, just each time, I have to struggle a little bit, for I always forget install some libraries.</p>

<p>So, record things here</p>

<p>I found ceph.com have also detailed its &ldquo;Building from Codes&rdquo; doc, indeed, much detailed than previous version</p>

<p>But seems I still missing some packages when doing &ldquo;make&rdquo;</p>

<p><code><a href="http://ceph.com/docs/master/install/build-ceph/">http://ceph.com/docs/master/install/build-ceph/</a></code></p>

<p>```console pre-make-ceph-packages
sudo apt-get install pkg-config autotools-dev autoconf automake cdbs gcc g++ git libboost-dev libedit-dev libssl-dev libtool libfcgi libfcgi-dev libfuse-dev linux-kernel-headers libcrypto++-dev libcrypto++ libexpat1-dev libboost-thread-dev uuid-dev libkeyutils-dev libgoogle-perftools-dev libatomic-ops-dev libaio-dev libgdata-common libgdata13 libsnappy-dev libleveldb-dev libboost-program-options-dev
cd ceph
./autogen.sh
./configure
make
make install</p>

<p>```</p>

<p><strong><strong><strong><strong><strong><strong><strong><strong><em>Extra parts</em></strong></strong></strong></strong></strong></strong></strong></strong>****
I have ran into the libboost-thread and libboost-system problem for really a lot of times, so the best way to do here is that:</p>

<h2>Keep same and the only version of libboost dpkg installed in your system ( verified by dpkg -l | grep boost )</h2>

<h2>Any time after your purge/remove and install pkg, don&rsquo;t just try make, you need to do ./configure first</h2>

<h2>Oh, maybe problem also can be caused by you have multi gcc/g++ version</h2>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to do Ceph Latency Breakdown]]></title>
    <link href="http://xuechendi.github.io/blog/2013/12/05/how-to-do-ceph-latency-breakdown/"/>
    <updated>2013-12-05T10:38:00+08:00</updated>
    <id>http://xuechendi.github.io/blog/2013/12/05/how-to-do-ceph-latency-breakdown</id>
    <content type="html"><![CDATA[<h2>The Ceph Latency Breakdown process includes 3 steps:</h2>

<h3>1. Enable Log Configuration or Other performance count mechanism to record the runtime infomation.</h3>

<h3>2. Produce the workload and meanwhile dump the perfcounter</h3>

<h3>3. Parse the perfcounter dump data</h3>

<h2>Details:</h2>

<p>In ceph, a perfcounter mechanism has been adopted to do the latency breakdown, and the runtime info can be dumped by the admin_socket is an internal implementation in Ceph.</p>

<p>example:</p>

<p><1> codes
``` c git diff
diff &mdash;git a/src/osd/OSD.cc b/src/osd/OSD.cc
index d34bb9c..6c308d3 100644
&mdash;&ndash; a/src/osd/OSD.cc
+++ b/src/osd/OSD.cc
@@ -7055,10 +7055,10 @@ PGRef OSD::OpWQ::_dequeue()</p>

<p> void OSD::OpWQ::_process(PGRef pg, ThreadPool::TPHandle &amp;handle)
 {
&ndash;  utime_t pg_lock_lat = ceph_clock_now(g_ceph_context);
+  utime_t start = ceph_clock_now(g_ceph_context);
   pg->lock_suspend_timeout(handle);
&ndash;  pg_lock_lat = ceph_clock_now(g_ceph_context) &ndash; pg_lock_lat;
&ndash;  osd->logger->tinc(l_osd_pg_lock_lat, pg_lock_lat);
+  utime_t end = ceph_clock_now(g_ceph_context);
+  osd->logger->tinc(l_osd_pg_lock_lat, end &ndash; start);</p>

<p>   OpRequestRef op;
   {
```</p>

<p>Current Ceph Version only enabled admin_socket in Ceph Cluster Side, so we need to enable it in RBD side.</p>

<p><2> dump the data</p>

<p>``` bash ceph.conf</p>

<h1>Enable the admin_socket in QEMU RBD</h1>

<p>check ceph.conf
perf = false
&ndash;></p>

<h1>perf = false</h1>

<p>```</p>

<p>```xml QEMU disk.xml</p>

<pre><code>&lt;disk type='network' device='disk'&gt;
  &lt;driver name='qemu' type='raw' cache='none'/&gt;
  &lt;source protocol='rbd' name='xcd_8osd/volume-3:admin_socket=/var/run/ceph/node5_1.asok'/&gt;
  &lt;target dev='vdb' bus='virtio'/&gt;
  &lt;serial&gt;12f70341-d199-4fca-9270-56e5d6b80061&lt;/serial&gt;
  &lt;alias name='virtio-disk1'/&gt;
  &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/&gt;
&lt;/disk&gt;
</code></pre>

<p><code>
</code> console
virsh attach instance1 disk.xml</p>

<p>ceph &mdash;admin-daemon /var/run/ceph/${osd} perf dump >> ${osd}.txt
```</p>

<hr>


<h2>Below is the summary of all scripts:</h2>

<p><a href ="/downloads/Ceph_Latency_Breakdown_Script.zip"><code style="font-family: 'Fjalla One','Georgia','Helvetica Neue',Arial,sans-serif;font-weight:900;font-size:12px;">download the script</code></a></p>

<h3>Script to mkceph:(in the dir &ldquo;mkceph&rdquo;)</h3>

<p>mkceph_with_pg.sh</p>

<h3>Script to produce workload:(all put in the dir &ldquo;run the test&rdquo;)</h3>

<p><b>run.sh</b>: aim to change the <code>ceph.conf</code> op thread number setting, then restart ceph, vm, and call <code>test.sh</code> to start the test. The test result will then be stored in the path &ldquo;/data/xcd/ceph-CephXCD/run_${id}&rdquo;</p>

<p><b>test.sh</b>: aim to clean run a vm_num loadline test, call <code>mhost-volume-test-xcd.sh</code> to run the actual test</p>

<p><b>mhost-volume-test-xcd.sh</b>: calls <code>clean_cache_on_ceph.sh</code>,<code>check_readahead.sh</code>, <code>volume-test_in_vm.sh</code>, <code>all.fio</code>, <code>volume-test_in_pc.sh</code>, <code>ceph_perf_counter_dump.sh</code>, <code>volume-test_in_ceph.sh</code>, <code>check_cephHealth.sh</code> to run the test(producing workload and collecting runtime system info &amp; ceph info), then collects all the data back to the Node1.
```console</p>

<h1>example:</h1>

<p>./mhost-volume-test-xcd.sh 1 $vm_num 3 4k &ldquo;vmNum_&rdquo;$vm_num ${qd} node13_net
```
tip: There is a VM list stored in file &ldquo;node13_net&rdquo;</p>

<h3>Script to parse the data:</h3>

<p><b>getLatbreakdown.sh</b>: aim to parse latency data from <code>*.asok</code> files using <code>post_perf.py</code></p>

<p><b>getResult.sh</b>: aim to parse iostat, mpstat data. It calls <code>post_volume_xcd.sh</code>, <code>post_osd.sh</code>, <code>post_cpu.sh</code>, <code>iostat.sh</code></p>
]]></content>
  </entry>
  
</feed>
