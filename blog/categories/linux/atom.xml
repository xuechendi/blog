<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Linux | XueChendi]]></title>
  <link href="http://xuechendi.github.io/blog/categories/linux/atom.xml" rel="self"/>
  <link href="http://xuechendi.github.io/"/>
  <updated>2013-12-10T16:35:27+08:00</updated>
  <id>http://xuechendi.github.io/</id>
  <author>
    <name><![CDATA[Chendi.Xue]]></name>
    <email><![CDATA[xuechendi@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Heartbeat and DRBD internal]]></title>
    <link href="http://xuechendi.github.io/blog/2013/12/09/heartbeat-and-drbd-internal/"/>
    <updated>2013-12-09T10:20:00+08:00</updated>
    <id>http://xuechendi.github.io/blog/2013/12/09/heartbeat-and-drbd-internal</id>
    <content type="html"><![CDATA[<p>This posting is talking about Heartbeat &amp; DRBD internal</p>

<p>Every high−availability system needs two basic services: to be informed of cluster members joining and leaving the system, and to provide basic communication services for managing a cluster.</p>

<p>Below info is quoted from <a href="http://upcommons.upc.edu/pfc/bitstream/2099.1/4970/1/memoria.pdf" target="_blank">MASTER THESIS of Abraham Iglesias Aparicio</a></p>

<h2>The Linux-HA project: Heartbeat</h2>

<p>Linux-ha project (Linux High Availability project) [5] is an open source suite to build high available clusters. It can run on every known Linux flavor and in some Unices such as FreeBSD and Solaris.
Heartbeat software is the core component of the Linux-HA project and permits building high availability clusters from commodity servers, as it has no hardware dependencies.</p>

<p>The project started in 1998 and is maintained by an open developer’s community. The project is lead by some companies like Novell or IBM. 10 years of development, testing and more than 30.000 production clusters are numbers that would reflect project maturity.</p>

<h3>Heartbeat cluster styles</h3>

<p>There are two different scopes when working with Heartbeat. Two styles of clusters can be configured. Let us say R1 cluster style and R2 cluster style.
R1 cluster style was the first kind of configurations that could be done with Heartbeat. In this configuration there were some limitations such as:</p>

<h5>1. Limitation of nodes in the cluster (it can only accept 2 nodes)</h5>

<h5>2. Cannot perform monitor operations of a cluster resource</h5>

<h5>3. There were almost no options to express dependency information</h5>

<br>


<p>First version of this R2 cluster style was 2.0.0. At the time of writing this document, Heartbeat latest stable version was 2.0.8. Therefore, this version will be the reference of study in this project. R2 is fully compatible with R1 cluster style configuration. It supports up to 16 nodes cluster and an asynchronous logging daemon was added. Moreover, there were some improvements on message architecture.
With R2 cluster style, or next generation of Heartbeat, these limitations were overridden and software architecture changed. Adding more components and functionalities lead to a more complex system but more complete than the R1 cluster style.
Some new components appeared with R2:</p>

<h5>1. ClusterInformationBase</h5>

<h5>2. ClusterResourceManager</h5>

<h5>3. Modular PolicyEngine</h5>

<h5>4. LocalResourceManager</h5>

<h5>5. StonithDaemon</h5>

<br>


<h4>ClusterInformationBase</h4>

<p>ClusterInformationBase (aka CIB) is the component which stores information about the cluster. It is replicated on every node and stores two kind of information:</p>

<h5>1. Static information. It includes definitions of cluster nodes, resources, monitor operations and constraints.</h5>

<h5>2. Dynamic information. CIB stores the current status of the cluster.</h5>

<br>


<p>CIB is formatted in a XML document and must be built following DTD document included with Heartbeat specifications. Cib.xml files can be red in annex 2.1.</p>

<h4>ClusterResourceManager</h4>

<p>The ClusterResourceManager (CRM) component consists on the following components:</p>

<h5>1. ClusterInformationBase</h5>

<h5>2. ClusterResourceManagerDaemon</h5>

<h5>3. PolicyEngine</h5>

<h5>4. Transitioner</h5>

<h5>5. LocalResourceManager</h5>

<br>


<p>The ClusterResourceManagerDaemon (CRMD) runs on every node and coordinates the actions of all other cluster resource managers. It exchanges information with the designated coordinator (DC) and it can be seen as a communication interface between DC and subsystem components such as CIB and LocalResourceManager.</p>

<p>The DC is a special instance of CRMD. It is elected with the help of the list of cluster nodes from the ClusterConsensusManager (CCM) and takes on the responsibilities of ResourceAllocation throughout the cluster. Therefore, DC is the one who delivers CIB information to all slave instances of CRMD.</p>

<p>The PolicyEngine (PE) module is the one who performs computation of the next state in the cluster. It is triggered by any event in the cluster, such as a resource changing state, a node leaving or joining, or CRM event such as a CRM parting or leaving. The PE, takes the input from CIB and runs locally and does not do any network I/O, it only runs an algorithm. It describes the actions and their dependencies necessary to go from the current cluster state to the target cluster state.
When PE returns an action it is passed to the Transitioner. The goal of this component is to make PE’s wishes become true. The new state computed by PE is passed to the Transitioner, which communicates with the LocalResourceManager on every node and informs about the actions decided by PE (start, stop resources).</p>

<h3>ClusterConsensusManager</h3>

<p>The ClusterConsensusManager (CCM) is the one who establishes membership in the cluster. This membership layer is in the middle of Heartbeat, which is the messaging and infrastructure layer, and the new CRM which is the resource allocation layer. CCM information includes new additions to the membership, lost nodes and loss of quorum.</p>

<h3>Stonith daemon</h3>

<p>R2 cluster style features include stonith capabilities. Stonith is an acronym meaning “Shot The Other Node In The Head”.
A stonith daemon is running on every node and receives requests from the CRM’s TE about what and when to reset. An example of when stonith daemon action is used is in a split brain situation. The DC will send a message to the Stonith daemon to reset the sick node. This node will reboot and then will join the cluster again in a healthy way.</p>

<h3>LocalResourceManager</h3>

<p>Finally, the LocalResourceManager (LRM) has the responsibility for performing operations on resources, by using ResourceAgent scripts to carry out the work. ResourceAgent scripts define a set of start, stop or monitor operations, as directed by the CRM, which are used to operate on a resource instance.
LRM also provides information about resources. It can provide a list of resources that are currently primary and its current state.</p>

<h3>Configuration and manageability</h3>

<p>Heartbeat GUI is a user interface to manage Heartbeat clusters. It is delivered in a separate package of Heartbeat core. It is an application developed with python and is able to add, remove nodes or cluster resources.
It is necessary to have connectivity to port 5560/tcp of the cluster node and password for hacluster user running on the cluster nodes.
Heartbeat GUI is executed from a shell and after being authenticated, the main screen of Heartbeat GUI appears. This is how GUI looks like:</p>

<p><img src="/images/HA/heartbeat_gui.png" alt="heartbeat_gui" /></p>

<h2>HA overview</h2>

<p><img src="/images/HA/pcmk-overview.png" alt="heartbeat_gui" /></p>

<h2>HA(Pacemaker) Internal Components</h2>

<p>Pacemaker itself is composed of four key components (illustrated below in the same color scheme as the previous diagram):</p>

<h5>Heartbeat：将原来的消息通信层独立为heartbeat项目，新的heartbeat只负责维护集群各节点的信息以及它们之前通信；</h5>

<h5>Cluster Glue：相当于一个中间层，它用来将heartbeat和pacemaker关联起来，主要包含2个部分，即为LRM和STONITH。</h5>

<h5>Resource Agent：用来控制服务启停，监控服务状态的脚本集合，这些脚本将被LRM调用从而实现各种资源启动、停止、监控等等。</h5>

<h5>Pacemaker:也就是Cluster Resource Manager （简称CRM），用来管理整个HA的控制中心，客户端通过pacemaker来配置管理监控整个集群。</h5>

<h5>Corosync:a Group Communication System， helps Pacemaker to support popular open source cluster filesystems.</h5>

<pre><code>what is Corosync:
    Corosync包含OpenAIS的核心框架用来对Wilson的标准接口的使用、管理。它为商用的或开源性的集群提供集群执行框架。Corosync执行高可用应用程序的通信组系统，它有以下特征：
    一个封闭的程序组（A closed process group communication model）通信模式，这个模式提供一种虚拟的同步方式来保证能够复制服务器的状态。
    一个简单可用性管理组件（A simple availability manager），这个管理组件可以重新启动应用程序的进程当它失败后。
    一个配置和内存数据的统计（A configuration and statistics in-memory database），内存数据能够被设置，回复，接受通知的更改信息。
    一个定额的系统（A quorum  syste）,定额完成或者丢失时通知应用程序。
</code></pre>

<p><img src="/images/HA/pcmk-stack.png" alt="heartbeat_gui" /></p>

<h5>1. CIB (aka. Cluster Information Base)</h5>

<p>The CIB uses XML to represent both the cluster’s configuration and current state of all resources in the cluster. The contents of the CIB are automatically kept in sync across the entire cluster and are used by the PEngine to compute the ideal state of the cluster and how it should be achieved.</p>

<h5>2. CRMd (aka. Cluster Resource Management daemon)</h5>

<p>This list of instructions is then fed to the DC (Designated Co-ordinator). Pacemaker centralizes all cluster decision making by electing one of the CRMd instances to act as a master. Should the elected CRMd process, or the node it is on, fail… a new one is quickly established.</p>

<h5>3. PEngine (aka. PE or Policy Engine)</h5>

<p>The DC carries out the PEngine’s instructions in the required order by passing them to either the LRMd (Local Resource Management daemon) or CRMd peers on other nodes via the cluster messaging infrastructure (which in turn passes them on to their LRMd process).</p>

<p>The peer nodes all report the results of their operations back to the DC and based on the expected and actual results, will either execute any actions that needed to wait for the previous one to complete, or abort processing and ask the PEngine to recalculate the ideal cluster state based on the unexpected results.</p>

<h5>4. STONITHd</h5>

<p>In some cases, it may be necessary to power off nodes in order to protect shared data or complete resource recovery. For this Pacemaker comes with STONITHd. STONITH is an acronym for Shoot-The-Other-Node-In-The-Head and is usually implemented with a remote power switch. In Pacemaker, STONITH devices are modeled as resources (and configured in the CIB) to enable them to be easily monitored for failure, however STONITHd takes care of understanding the STONITH topology such that its clients simply request a node be fenced and it does the rest.</p>

<p><img src="/images/HA/pcmk-internals.png" alt="heartbeat_gui" /></p>

<p>A heartbeat is a periodic communication attempt to check for updated policies.</p>

<p>Heartbeat仅仅是个HA软件，它仅能完成心跳监控和资源接管，不会监视它控制的资源或应用程序。要监控资源和应用程序是否运行正常，必须使用第三方的插件，例如ipfail、Mon和Ldirector等。Heartbeat自身包含了几个插件，分别是ipfail、Stonith和Ldirectord，介绍如下。</p>

<p>ipfail的功能直接包含在Heartbeat里面，主要用于检测网络故障，并做出合理的反应。为了实现这个功能，ipfail使用ping节点或者ping节点组来检测网络连接是否出现故障，从而及时做出转移措施。</p>

<p>Stonith插件可以在一个没有响应的节点恢复后，合理接管集群服务资源，防止数据冲突。当一个节点失效后，会从集群中删除。如果不使用Stonith插件，那么失效的节点可能会导致集群服务在多于一个节点运行，从而造成数据冲突甚至是系统崩溃。因此，使用Stonith插件可以保证共享存储环境中的数据完整性。</p>

<p>Ldirector是一个监控集群服务节点运行状态的插件。Ldirector如果监控到集群节点中某个服务出现故障，就屏蔽此节点的对外连接功能，同时将后续请求转移到正常的节点提供服务。这个插件经常用在LVS负载均衡集群中。关于Ldirector插件的使用，将在后续章节详细讲述。</p>

<p>针对系统自身出现问题导致heartbeat无法监控的问题，就需要在Linux内核中启用一个叫watchdog的模块。watchdog是一个Linux内核模块，它通过定时向/dev/watchdog设备文件执行写操作，从而确定系统是否正常运行。如果watchdog认为内核挂起，就会重新启动系统，进而释放节点资源。</p>

<p>在Linux中完成watchdog功能的软件叫softdog。softdog维护一个内部计时器，此计时器在一个进程写入/dev/watchdog设备文件时更新。如果softdog没有看到进程写入/dev/watchdog文件，就认为内核可能出了故障。watchdog超时周期默认是一分钟，可以通过将watchdog集成到Heartbeat中，从而通过Heartbeat来监控系统是否正常运行。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[DRBD and Heartbeat]]></title>
    <link href="http://xuechendi.github.io/blog/2013/12/04/drbd-and-heartbeat/"/>
    <updated>2013-12-04T19:07:00+08:00</updated>
    <id>http://xuechendi.github.io/blog/2013/12/04/drbd-and-heartbeat</id>
    <content type="html"><![CDATA[<p>These days, need to deploy a HA solution in our setup, and just write all these down helping the next time deployment.</p>

<h2>The Overview</h2>

<p><img src="/images/HA/IMG_0045.PNG" alt="overview" width=80%></p>

<p>Here we have 2 setups, providing both web service for management and iscsi target service.</p>

<p>What we wanna have is all the LVs in setup1(222.31.76.144) has a mirror in setup2(222.31.76.228) &mdash; so we need DRBD</p>

<p>Then we use LV1 as lun0 to provide iscsi disk to client1, all client1 knows is that his iscsi target provider&rsquo;s ip address is 222.31.76.250(may be setup1 or setup2) &mdash; so we need heartbeat.</p>

<p>Basically, setup1 will response to the client&rsquo;s request, only when setup&rsquo;s eth0 nic doesn&rsquo;t work(this will happen when setup1 is down), setup2 will continue to provide web service and iscsi service.</p>

<p>Ok, another problem, when we need setup2 can continue to provide iscsi service, we need disks in setup to can also be the primary node in drbd semantic, so when data written to setup2, data can be sync to setup1.</p>

<p>Let&rsquo;s just summary all the requirement here:</p>

<ol>
<li><p>Using heartbeat to provide a virtual ip 222.31.76.250 and watch the liveness of both setups.</p></li>
<li><p>Using DRBD to sync LVs in setup1 and setup2, the syncronization can be both sides(dual primary).</p></li>
<li><p>Need to keep in mind, when the sync link(eth0,192.168.1.*) breaks, there will be a split brain situation in our DRBD setting, so we need to record who is the latest iscsi target provider and use that one to recover the DRBD setting.</p></li>
</ol>


<h2>DRBD setting</h2>

<p>1.The installation
we need to install two parts of DRBD, the kmod who is the real implementation of drbd, and the drbdadm, which helps a lot to manage the drbd service.</p>

<p><code>bash drbd rpm
drbd83-8.3.15-2.el5.centos.x86_64.rpm       kmod-drbd83-8.3.15-3.el5.centos.x86_64.rpm
</code></p>

<p>2.The drbd config file
We can start from the drbd conf file /etc/drbd.conf</p>

<p>``` bash /etc/drbd.conf
#</p>

<h1>please have a a look at the example configuration file in</h1>

<h1>/usr/share/doc/drbd83/drbd.conf</h1>

<p>#
include &ldquo;drbd.d/global_common.conf&rdquo;;
include &ldquo;drbd.d/*.res&rdquo;;
```</p>

<p>As we can see, in this config file,it include a global_common.conf and *.res
The first one writes some common setting for drbd, things like the sync protocol(Async, semi-sync, sync), net rate setting, blah, blah&hellip;
The second one is what we need to build our resource, the config will be like below:</p>

<p>``` bash mpx.res
resource mpx {</p>

<pre><code>protocol C; 
net {
    allow-two-primaries; 
  }
syncer {
      rate 1024M;  //sync bandwidth
 }
on super {
    device /dev/drbd0;
    disk /dev/raid0/lv01;
    address 192.168.1.100:7789;
    meta-disk internal;        
}
on MPXHA {
    device /dev/drbd0;
    disk /dev/vg01/lv01;
    address 192.168.1.152:7789;
    meta-disk internal;        
}
</code></pre>

<p>}
```</p>

<p>The protocol part says we will use sync method in this resource
The net part says both nodes can be primary
The last two parts describe the drbd device and the nic they used to sync data(Here we can direclty connect two setup by ethernet line.)</p>

<p>The above *.res should has replication both in setup1 and setup2
After that there is few command we need to first time build this DRBD resource.</p>

<p>``` bash first building drbd resource
drbdadm create-md mpx // this should be done both in setup1 and setup2
service drbd restart // both in setup1 and setup2</p>

<p>drbdadm primary &mdash;force resource // only in setup1</p>

<p>drbd-overview // can be done both in setup1 or setup2, here I did in setup2
drbd driver loaded OK; device status:
version: 8.3.11 (api:88/proto:86-96)
srcversion: 2D876214BAAD53B31ADC1D6
m:res  cs          ro                 ds
0:mpx  SyncTarget  Secondary/Primary  Inconsistent/UpToDate  C
&hellip;    sync'ed:    25.4%              (1373576/1837720)K</p>

<p>&hellip; wait until</p>

<p>drbd driver loaded OK; device status:
version: 8.3.11 (api:88/proto:86-96)
srcversion: 2D876214BAAD53B31ADC1D6
m:res  cs         ro                 ds                 p  mounted  fstype
0:mpx  Connected  Secondary/Primary  UpToDate/UpToDate  C</p>

<p>drbdadm primary mpx // in setup2
drbd-overview
drbd driver loaded OK; device status:
version: 8.3.11 (api:88/proto:86-96)
srcversion: 2D876214BAAD53B31ADC1D6
m:res  cs         ro               ds
0:mpx  Connected  Primary/Primary  UpToDate/UpToDate  C
```</p>

<p>Here is when we use drbd sevice as iscsi lun,yeah, you need this copy both in setup1 and setup2</p>

<p>``` bash scst.conf</p>

<h1>Automatically generated by SCST Configurator v2.2.0.</h1>

<p>HANDLER vdisk_fileio {</p>

<pre><code>DEVICE vg01_lv01 {
    filename /dev/drbd0 // ￼ drbd 
}
</code></pre>

<p>}
TARGET_DRIVER iscsi {</p>

<pre><code>enabled 1
TARGET iqn.2011-1212.edu.cuc.storagelab:mpx.target1 {
    cpu_mask 0000ff
    enabled 1
    rel_tgt_id 1
    GROUP xcd {
        LUN 0 vg01_lv01
        INITIATOR iqn.2012-12.cn.edu.cuc.xcd:1234567
        INITIATOR iqn.1991-05.com.microsoft:win-p56rroe0srf
        cpu_mask 0000ff
    }
} 
</code></pre>

<p>}
```</p>

<h2>Heartbeat setting</h2>

<ol>
<li>The installation
Seems we can not simply get heartbeat in yum
Here is the solution</li>
</ol>


<p>``` console heartbeat installation
Step-1 : Make download folder under the root.
[root@setup1 download]# mkdir /download
[root@setup1 download]# cd /download/
Step-2 : Download EPEL repository.
[root@setup1 download]# wget <a href="ftp://mirror.switch.ch/pool/1/mirror/scientificlinux/6rolling/i386/os/Packages/epel-release-6-5.noarch.rpm">ftp://mirror.switch.ch/pool/1/mirror/scientificlinux/6rolling/i386/os/Packages/epel-release-6-5.noarch.rpm</a>
Step-3 : Install Epel RPM.
rpm -ivUh epel-release-6-5.noarch.rpm
warning: epel-release-6-5.noarch.rpm: Header V4 DSA/SHA1 Signature, key ID 192a7d7d: NOKEY
Preparing&hellip;                ########################################### [100%]
   1:epel-release           ########################################### [100%]
Step-4 : Edit epel.repo file ‘/etc/yum.repos.d/epel.repo’ change the line # 6 ‘enable=1 to enable=0′.
[root@setup1 download]# vi /etc/yum.repos.d/epel.repo
[epel]
name=Extra Packages for Enterprise Linux 6 &ndash; $basearch</p>

<h1>baseurl=<a href="http://download.fedoraproject.org/pub/epel/6/$basearch">http://download.fedoraproject.org/pub/epel/6/$basearch</a></h1>

<p>mirrorlist=<a href="http://mirrors.fedoraproject.org/metalink?repo=epel-6&amp;amp;arch=$basearch">http://mirrors.fedoraproject.org/metalink?repo=epel-6&amp;amp;arch=$basearch</a>
failovermethod=priority
enabled=0
CentOS 6 :<br/>
Step-5 : Now we are ready to install Heartbeat with yum command.
[root@setup1 download]# yum &mdash;enablerepo=epel install heartbeat
```</p>

<ol>
<li>The setting</li>
</ol>


<p><code>console
[root@setup1&amp;setup2 download]# cp /usr/share/doc/heartbeat-3.0.4/ha.cf /etc/ha.d/
[root@setup1&amp;setup2 download]# cp /usr/share/doc/heartbeat-3.0.4/haresources /etc/ha.d/
[root@setup1&amp;setup2 download]# cp /usr/share/doc/heartbeat-3.0.4/authkeys /etc/ha.d/
[root@setup1 download]# vim /etc/ha.d/ha.cf
debugfile /var/log/ha-debug
keepalive 2
deadtime 10
warntime 6
initdead 120
udpport 694
ucast eth0 222.31.76.228
node   super
node   MPXHA
auto_failback on
respawn hacluster /usr/lib64/heartbeat/ipfail
[root@setup2 download]# vim /etc/ha.d/ha.cf
debugfile /var/log/ha-debug
keepalive 2
deadtime 10
warntime 6
initdead 120
udpport 694
ucast eth0 222.31.76.144
node   super
node   MPXHA
auto_failback on
respawn hacluster /usr/lib64/heartbeat/ipfail
[root@setup1&amp;setup2 download]# vim /etc/ha.d/haresouces
super IPaddr::222.31.76.250/24/eth0
[root@setup1&amp;setup2 download]# service heartbeat restart
</code></p>

<h2>There is also one problem we mentioned before called &ldquo;split brain&rdquo;, here is the way to solve this situation</h2>

<p>This situation happens when the eth0(the link used to sync data beween drbd resources are broken), and both drbd will see itself as primary and the other as unknown</p>

<p><code>console when split brain happens
[root@super ~]# service drbd status
drbd driver loaded OK; device status:
version: 8.3.11 (api:88/proto:86-96)
srcversion: 2D876214BAAD53B31ADC1D6
m:res  cs          ro               ds                 p       mounted  fstype
0:mpx  StandAlone  Primary/Unknown  UpToDate/DUnknown  r-----
</code></p>

<p>The way to tackle this problem when both drbd device also act as the iscsi lun</p>

<p>```console How to solve split brain problem
service scst stop
drbdadm secondary mpx
drbdadm disconnect mpx
drbdadm &mdash; &mdash;discard-my-data connect mpx
ssh 222.31.76.144 &ldquo;drbdadm connect mpx&rdquo;</p>

<p>service drbd status</p>

<p>drbd driver loaded OK; device status:
version: 8.3.11 (api:88/proto:86-96)
srcversion: 2D876214BAAD53B31ADC1D6
m:res  cs          ro                 ds
0:mpx  SyncTarget  Secondary/Primary  Inconsistent/UpToDate  C
&hellip;    sync'ed:    25.4%              (1373576/1837720)K</p>

<p>drbd driver loaded OK; device status:
version: 8.3.11 (api:88/proto:86-96)
srcversion: 2D876214BAAD53B31ADC1D6
m:res  cs         ro                 ds                 p  mounted  fstype
0:mpx  Connected  Secondary/Primary  UpToDate/UpToDate  C</p>

<p>drbdadm primary mpx</p>

<p>drbd driver loaded OK; device status:
version: 8.3.11 (api:88/proto:86-96)
srcversion: 2D876214BAAD53B31ADC1D6
m:res  cs         ro               ds
0:mpx  Connected  Primary/Primary  UpToDate/UpToDate  C</p>

<p>service scst start
```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[System analysis and tools]]></title>
    <link href="http://xuechendi.github.io/blog/2013/11/15/system-analysis-and-tools/"/>
    <updated>2013-11-15T01:09:00+08:00</updated>
    <id>http://xuechendi.github.io/blog/2013/11/15/system-analysis-and-tools</id>
    <content type="html"><![CDATA[<h2>It&rsquo;s really clear of these 2 graphs to remind me all those tools can help to analyse the system</h2>

<h2>So happily sharing here ~~~</h2>

<p><img src="/images/system_analysis_tools/overview.jpg" alt="overview" />
<img src="/images/system_analysis_tools/tools.jpg" alt="overview" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Device-Mapper deep dive]]></title>
    <link href="http://xuechendi.github.io/blog/2013/11/14/device-mapper-deep-dive/"/>
    <updated>2013-11-14T19:40:00+08:00</updated>
    <id>http://xuechendi.github.io/blog/2013/11/14/device-mapper-deep-dive</id>
    <content type="html"><![CDATA[<p>I used to stuck in all these terms and concepts for like really long time, just trying to get everything clear by writing.</p>

<h2>Storage subsystem</h2>

<h3>Overview</h3>

<p><img src="/images/dm_deep_dive/overview.png" alt="overview" width=80%></p>

<p>BIO is the unit to map data in the memory to generic block offset.
When generic block layer gets bio units, it calls io scheduler to combine bios into request to specific device.
Then requests can be sent to real device or virtual block device like software raid or logic volume(using MD or Device Mapper modules).</p>

<p><img src="/images/dm_deep_dive/bio.png" alt="bio" width=80%></p>

<p>Actually BIO units point to a much smaller unit named bio_vec which is the exactly unit point to the memory, and BIO also has one field record which block device and which sector it wanna to read/write.(Notice, the block device here is kind of a generic idea, could be some virtual block device)</p>

<p>The smart use of bio_vec help kernel to support scatter/Gather I/O, so that BIO can map some scatter part in mem to some continuous part in block device.</p>

<p><img src="/images/dm_deep_dive/request.png" alt="request" width=80%></p>

<p>When BIO unit received by generic block layer, kernel will do some &ldquo;merge and sort&rdquo; operations then hand the combined BIOs to block device. All these work can be done in the IO scheduler layer and then all BIO units are combined into one and one request, which also be linked by a pointer named &ldquo;request_queue&rdquo; store in bdev struct(gendisk).</p>

<p>Then the whole idea is pretty clear, there is a picture shows some important function to translate a fs syscall into requests to block devices.</p>

<p><img src="/images/dm_deep_dive/functions.png" alt="functions" width=50%></p>

<p>Submit_bio is a generic api to submit bio to generic block layer(of course by its name&hellip;)</p>

<p>generic_make_request puts BIO into bio_list, then __generic_make_request will see if this bio is suitable to make request or it is delivered to some stack device like Device Mapper(in this situation, __generic_make_request will produce a new bio and call generic_make_request).</p>

<p>__make_request_fn() then pus BIO into request_queue, if this function returns 0, the BIO is delivered to the real block device, or it may continues to call __make_request_fn until it delivered to real block device(like Device Mapper).</p>

<p>In fact, the request_queue also will not be directly tackled by block device, the device will use some method named &ldquo;Plugging/Unplugging&rdquo; to tackles these requests.</p>

<p><img src="/images/dm_deep_dive/structures.png" alt="structures" width=50%></p>

<p>Now let&rsquo;s see a linked graph of some important structs in block-level subsystem. Every block device has a field named gendisk(generic disk), and generic disk has a field to record its request_queue, and since each type of block device has its own implementation of read and write, the gendisk also has a field named private_data to point to the corresponding block device(also, the block device can be real or virtual).</p>

<h2>Then, after all the general idea of block-layer subsystem, let&rsquo;s talk about <code>Device Mapper</code></h2>

<h3>What is Device-Mapper</h3>

<ul>
<li>A block device mapping facility available in Linux Kernel.</li>
<li>A component required by LVM2 to support the mapping between logical volumes and physical storage devices.</li>
<li>Device-mapper provides a generic way to create virtual layers of block devices that can do different things on top of real block devices like striping, concatenation, mirroring, snapshotting, etc&hellip;</li>
</ul>


<h3>Here is the Usecase</h3>

<p>Before lv creation</p>

<p><img src="/images/dm_deep_dive/Before lv creation.png" alt="Before lv creation" width=40%></p>

<p>After lv creation</p>

<p><img src="/images/dm_deep_dive/after lv creation.png" alt="after lv creation" width=40%></p>

<p>The thing should notice here is that a dm-0 device in /sys/block is created, which indicates that Logic Volume is a &ldquo;device mapper&rdquo; device.</p>

<p><img src="/images/dm_deep_dive/after lv creation2.png" alt="after lv creation" width=40%></p>

<h3>another device mapper usecase</h3>

<p><img src="/images/dm_deep_dive/dm_usecase2.png" alt="dm_usecase2" width=60%></p>

<h3>Here is the Device Mapper Overview</h3>

<p><img src="/images/dm_deep_dive/dm_overview.png" alt="dm_overview" width=50%></p>

<p><img src="/images/dm_deep_dive/dm_struct.png" alt="dm_struct" width=50%></p>

<p>DM Devices in Ubuntu 12.10,kernel 3.6.3</p>

<p><img src="/images/dm_deep_dive/dm_targets.png" alt="dm_targets" width=50%></p>

<p>Linear Device target-type example</p>

<p>``` c linear-device-struct
static struct target_type linear_target = {   </p>

<pre><code>.name   = "linear",   
.version = {1, 1, 0},   
.module = THIS_MODULE,   
.ctr    = linear_ctr,   
.dtr    = linear_dtr,   
.map    = linear_map,   
.status = linear_status,   
.ioctl  = linear_ioctl,   
.merge  = linear_merge,   
.iterate_devices = linear_iterate_devices,
</code></pre>

<p>}
```</p>

<h3>Let&rsquo;s see the codes</h3>

<p>``` c How DM handle the device creation command?
int dm_create(int minor, struct mapped_device <em>*result){
     struct mapped_device </em>md;</p>

<pre><code>    md = alloc_dev(minor);       
if (!md)             
    return -ENXIO;     
dm_sysfs_init(md);       
*result = md;    
return 0;
</code></pre>

<p>}
```</p>

<p>``` c How DM handle the read/write command?
static void dm_request(struct request_queue <em>q, struct bio </em>bio)
{</p>

<pre><code>struct mapped_device *md = q-&gt;queuedata;

if (dm_request_based(md))
    blk_queue_bio(q, bio);  //Using dm_target rules to reconstruct the bio
else
    _dm_request(q, bio);    //split and process this bio
</code></pre>

<p>}
```</p>
]]></content>
  </entry>
  
</feed>
